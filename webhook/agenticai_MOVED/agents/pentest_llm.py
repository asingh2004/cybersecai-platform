# 1. What Your Pentest LLM Agent Does
# Your Pentest LLM agent is an automated penetration testing orchestrator that:

# Accepts a domain name (target).
# Performs passive & active reconnaissance, including:
# DNS assessment: Checks multiple DNS record types (A, PTR, MX, DMARC, SPF, TXT, AAAA).
# Port scanning: Scans common TCP ports (Nmap top 1000) and fingerprints services (service/product/version info).
# HTTP analysis: Probes the web server with multiple HTTP methods (GET, HEAD, OPTIONS), collects SSL/TLS details, detects cookies, login panels, WAF (Web Application Firewall), and attempts technology stack fingerprinting (Apache, nginx, PHP, etc).
# Vulnerability search: Checks identified technologies and ports against:
# CIRCL.lu (CVE database)
# Vulners.com (if API key given)
# Shodan (if API key given)
# Summarizes findings and presents them:
# As a Markdown report with an executive summary, findings table, remediation guidance, narrative attack surface, and appendices.
# 2. What is "Fuzzy Logic" in This Context?
# In classical computing, “fuzzy logic” refers to reasoning with approximate/truth values between 0 and 1 (instead of just True/False). Here, however, "fuzzy" means the reasoning is not just rule-based or binary, but involves LLM-driven interpretation, explanation, and triage.

# Where?

# Any place your agent needs to explain findings, rate severity, or summarize risks, it goes beyond simple “if/then rules.”
# It uses LLM (Large Language Model) reasoning to turn raw scan/output into executive-level explanations or recommendations—that is the “fuzzy” part.
# Example: The agent calls llm_instruct(<evidence>, <goal>) to have GPT reason about:
# What does this finding mean?
# How should it be expressed for a non-technical reader?
# What is the business impact/implication?
# Why is this a risk, and how severe?
# How should it be remediated?
# This is inherently “fuzzier” than simple if/else logic because the LLM can interpret context, choose words, express nuance, and summarize multiple findings for clarity.

# 3. Role of the LLM (Large Language Model, e.g., GPT-4)
# The LLM’s role is:
# Interpret raw output from scanners/recon tools (DNS/Nmap/HTTP/CVE) and put it in readable, actionable language.
# Classify and triage risk: Provide human-like explanations for why something is high/medium/low risk, or how an attacker might exploit it.
# Generate executive/board-ready summaries: Your “Executive Summary” bullet points are generated by the LLM based on recent evidence.
# Tailor remediation: It suggests prioritized actions or “quick wins” for improvements, based on what it understands from the scans.
# Explain findings in plain English: Each CVE/finding/port/service/technology can have a custom, context-aware explanation for a non-technical audience.
# Fuzzy/flexible context: If there’s ambiguity in scan output, the LLM can reason about what might be happening, why a result matters, or what else to check next.
# In code:
# The function llm_instruct(findings_so_far, goal) builds a specially crafted prompt, which is sent to the LLM (via OpenAI API).
# The LLM generates Markdown for executive summaries, explanations, impacts, and recommendations.
# The tool orchestrates gathering evidence, then lets the LLM "fuzzily" interpret and narrate the findings.

import re
import os
import json
import socket
import requests
from datetime import datetime, timezone
import httpx
import dns.resolver
import dns.reversename
import ssl
import platform
import random
from urllib.parse import urlparse, urljoin, parse_qs
from typing import Dict, Any, List, Tuple

# External dependencies expected:
# - dnspython
# - httpx
# - python-nmap (optional; handled gracefully)
# - A configured LLM client in config.client compatible with OpenAI Chat API (model: gpt-4.1)
# Fallback LLM shim provided if config.client is unavailable
try:
    from config import client  # LLM client required
except Exception:
    class _LLMMessage:
        def __init__(self, content=""):
            self.content = content

    class _LLMChoice:
        def __init__(self, content=""):
            self.message = _LLMMessage(content=content)

    class _LLMResp:
        def __init__(self, content=""):
            self.choices = [_LLMChoice(content=content)]

    class _LLMClientShim:
        class chat:
            class completions:
                @staticmethod
                def create(model, messages, temperature=0.0):
                    # Minimal deterministic shim to ensure code keeps running without a configured LLM.
                    # Produces concise responses to keep flow intact.
                    sys_msg = (messages[0]["content"] if messages else "").lower()
                    user_msg = (messages[-1]["content"] if messages else "")
                    if "stack" in sys_msg:
                        return _LLMResp(content="Stack: Unknown")
                    if "json" in sys_msg:
                        # Support/Vuln checker shim
                        try:
                            # extract component/version roughly from user_msg
                            cm = re.search(r'component:\s*"([^"]+)"', user_msg, re.I)
                            vm = re.search(r'version:\s*"([^"]+)"', user_msg, re.I)
                            comp = cm.group(1) if cm else "Unknown"
                            ver = vm.group(1) if vm else ""
                        except Exception:
                            comp, ver = "Unknown", ""
                        data = {
                            "component": comp, "version": ver,
                            "support_status": "Unknown", "known_vulnerabilities": [], "notes": ""
                        }
                        return _LLMResp(content=json.dumps(data))
                    return _LLMResp(content="OK")
    client = _LLMClientShim()

try:
    from utils.logging import log_to_laravel
except Exception:
    def log_to_laravel(section: str, detail: str):
        # silent no-op fallback
        pass

# =============================== Constants and Configuration ===============================
TAG_AUTOMATABLE = "Automatable"
TAG_ASSISTED = "Assisted"
TAG_MANUAL = "Manual"

CONDENSED_MODE = True

# Commercial enhancements toggles
PRODUCT_NAME = "SurfaceProber"
VENDOR_NAME = "cybersecai.io"
ENABLE_RISK_SCORE = True
ENABLE_BRAND_FRIENDLY = True

# HTTP behavior tuning for better stack detection
HTTP_TIMEOUT = 15
MAX_REDIRECTS = 10
HTML_MAX_BYTES = 600000  # increased cap to allow heavier landing pages for stack/version detection
USER_AGENT_POOL = [
    # Modern Chrome UAs across OSes to minimize bot-blocks
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)"
    " Chrome/125.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_4) AppleWebKit/605.1.15 (KHTML, like Gecko)"
    " Version/16.4 Safari/605.1.15",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko)"
    " Chrome/124.0.0.0 Safari/537.36",
]

# Safe version probing (non-intrusive endpoints) – improves CMS version accuracy
SAFE_VERSION_PROBES = True
VERSION_PROBE_PATHS = {
    "Drupal": ["/core/CHANGELOG.txt", "/CHANGELOG.txt"],
    "WordPress": ["/readme.html"],  # /wp-includes/version.php is executed server-side; avoid probing PHP files
    "Joomla": ["/administrator/manifests/files/joomla.xml"],
}
MAX_PROBE_BYTES = 50000  # cap small auxiliary fetches

PROMPT_LLMI_INSTRUCT_SYSTEM = "You are a professional pentester/report writer."
PROMPT_LLMI_INSTRUCT_USER_TEMPLATE = """You are a world-class penetration tester.

Context:
{context}

Goal:
{goal}

Instructions:
- Be concise and actionable.
- Prefer bullet points and short paragraphs.
- Use only the provided evidence/context; do not invent data.
- If evidence is insufficient, state the limitation explicitly.
"""

PROMPT_SUPPORT_VULN_SYSTEM = "You are a meticulous security analyst. Only output valid JSON as instructed."
PROMPT_SUPPORT_VULN_USER_TEMPLATE = """Return ONLY minified JSON. You are assessing software lifecycle support and notable vulnerabilities.
- knowledge_cutoff: 2024-10
- today: {today}
- If unsure, set support_status to "Unknown".
- Do NOT invent CVE IDs; include only well-known ones you are reasonably confident about, else return an empty list.
- Keep notes concise (<=180 chars).

Input:
component: "{component}"
version: "{version}"
context: "{context}"

Respond with JSON having keys:
component, version, support_status (Supported|Unsupported|EOL|Unknown),
known_vulnerabilities (array of objects with id and summary),
notes
"""

# Tighter tech LLM to avoid chatty disclaimers
PROMPT_TECH_SYSTEM = "You infer web stacks precisely. Output strictly: Stack: <item1>; <item2>; ..."
PROMPT_TECH_USER_TEMPLATE = """Infer the likely web technologies/frameworks from headers and first 800 chars of HTML.
- Return ONE line starting with: Stack:
- Items should be semicolon-separated, short names (e.g., WordPress; PHP; Nginx; Cloudflare).
- No explanations or extra text.

Headers (JSON):
{headers}

HTML (first 800 chars):
{html}

Previous info (optional):
{prev}
"""

# =============================== Helper: Detect Web Frameworks ===============================
def detect_frameworks_basic(headers: Dict[str, str], cookies: Dict[str, str]) -> str:
    # Legacy/basic detector retained as a fallback
    stack = []
    candidates = []
    if 'x-powered-by' in headers:
        candidates.append(headers['x-powered-by'].lower())
    if 'x-generator' in headers:
        candidates.append(headers['x-generator'].lower())
    if 'server' in headers:
        candidates.append(headers['server'].lower())
    for ck in cookies.keys():
        if ck.lower().startswith(('laravel', 'ci_session', 'php', 'symfony')):
            candidates.append(ck.lower())
    if any(re.search(r'laravel', c) for c in candidates):
        stack.append('Laravel (PHP)')
    if any(re.search(r'wordpress', c) for c in candidates):
        stack.append('WordPress (PHP)')
    if any(re.search(r'cakephp', c) for c in candidates):
        stack.append('CakePHP')
    if any(re.search(r'django', c) for c in candidates):
        stack.append('Django (Python)')
    if any(re.search(r'flask|python', c) for c in candidates):
        stack.append('Flask (Python)')
    if any(re.search(r'rails|ruby', c) for c in candidates):
        stack.append('Ruby on Rails')
    if any(re.search(r'node|express', c) for c in candidates):
        stack.append('Node.js/Express')
    if any(re.search(r'spring|tomcat|java', c) for c in candidates):
        stack.append('Java (Spring/Tomcat)')
    if any('php' in c for c in candidates):
        stack.append('PHP')
    if any(re.search(r'iis|asp\.net|aspnet|microsoft', c) for c in candidates):
        stack.append('.NET/IIS')
    if any(re.search(r'drupal|x-drupal-cache|x-drupal-dynamic-cache', c) for c in candidates):
        stack.append('Drupal (PHP)')
    return ', '.join(sorted(set(stack))) or "Unknown"

# Enhanced, commercial-grade stack fingerprinting (Wappalyzer-lite signatures)
CMS_SIGS = [
    # name, evidence regex, confidence
    ("WordPress", r'wp-content|wp-includes|<meta[^>]+generator[^>]+WordPress|x-pingback|wordpress', 0.95),
    ("Drupal", r'drupal|drupal-settings-json|<meta[^>]+generator[^>]+Drupal|/sites/default|x-drupal-cache|x-drupal-dynamic-cache|x-generator:\s*drupal', 0.97),
    ("Joomla", r'joomla|<meta[^>]+generator[^>]+Joomla', 0.9),
    ("Sitecore", r'sitecore|SC_ANALYTICS_GLOBAL_COOKIE|<meta[^>]+name="Generator"[^>]+Sitecore', 0.85),
    ("Adobe Experience Manager", r'adobe cq|aem|/etc\.clientlibs|Granite', 0.85),
    ("Squiz Matrix", r'squiz matrix|x-generator[^:]*:\s*Squiz Matrix|squiz\.net', 0.9),
    ("Shopify", r'x-shopify|shopify|cdn\.shopify|X-Shopify-Stage', 0.9),
    ("Magento", r'magento|x-magento|mage-cache|mage-cache-sessid', 0.9),
    ("Wix", r'\bx-wix\b|X-Seen-By|wix\.com', 0.85),
    ("Squarespace", r'squarespace|x-servedby:\s*squarespace', 0.85),
]

FRAMEWORK_SIGS = [
    ("Laravel (PHP)", r'laravel|laravel_session|x-laravel|x-powered-by:\s*php', 0.85),
    ("Symfony (PHP)", r'symfony|sf_redirect|x-symfony', 0.8),
    ("CodeIgniter (PHP)", r'ci_session|codeigniter', 0.75),
    ("Express (Node.js)", r'x-powered-by:\s*express|express', 0.95),
    ("Next.js (React)", r'__NEXT_DATA__|x-powered-by:\s*next\.js', 0.95),
    ("Nuxt.js (Vue)", r'__NUXT__|nuxt', 0.9),
    ("Ruby on Rails", r'x-runtime|x-request-id|_rails_|phusion passenger', 0.9),
    ("Django (Python)", r'csrftoken|sessionid;|django', 0.75),
    ("Flask (Python)", r'werkzeug|flask', 0.75),
    ("Spring (Java)", r'\bspring\b|jsessionid', 0.75),
    (".NET (ASP.NET/IIS)", r'iis|asp\.net|x-aspnet-version|x-aspnetmvc-version', 0.9),
]

SERVER_SIGS = [
    ("Nginx", r'nginx|openresty', 0.95),
    ("Apache", r'apache|httpd|apache/|x-powered-by:\s*apache', 0.9),
    ("LiteSpeed", r'litespeed', 0.9),
    ("Caddy", r'caddy', 0.8),
    ("IIS", r'iis|microsoft-iis', 0.95),
    ("Varnish", r'varnish', 0.85),
    ("AWS ALB/ELB", r'awselb/|x-amzn-', 0.8),
]

CDN_SIGS = [
    ("Cloudflare", r'cloudflare|cf-.*|server:\s*cloudflare', 0.95),
    ("Akamai", r'akamai|akamai-ghost', 0.9),
    ("Fastly", r'fastly|via:\s*1\.1 varnish|x-served-by', 0.9),
    ("Amazon CloudFront", r'cloudfront|x-amz-cf-', 0.9),
    ("Azure Front Door", r'azurefd|x-azure-ref', 0.85),
]

LANG_SIGS = [
    ("PHP", r'\bphp\b|x-powered-by:\s*php', 0.9),
    ("Node.js", r'node\.js|express|next\.js|nuxt|x-powered-by:\s*express', 0.9),
    ("Python", r'python|django|flask|werkzeug', 0.8),
    ("Java", r'java|spring|jsessionid|tomcat', 0.8),
    ("Ruby", r'ruby|rails|phusion passenger', 0.8),
    (".NET", r'\b\.net\b|asp\.net|iis', 0.9),
]

# Version extraction patterns (page-first approach)
META_GENERATOR_VERSION_PATS = [
    ("Drupal", r'(?i)\bdrupal\s*([0-9]+(?:\.[0-9]+){0,2})\b'),
    ("WordPress", r'(?i)\bwordpress\s*([0-9]+(?:\.[0-9]+){0,2})\b'),
    ("Joomla", r'(?i)\bjoomla!?[^0-9]*([0-9]+(?:\.[0-9]+){0,2})\b'),
    ("TYPO3", r'(?i)\btypo3\s*([0-9]+(?:\.[0-9]+){0,2})\b'),
]

ASSET_VERSION_PATS = [
    # WordPress assets often include the WP core version as ?ver=...
    ("WordPress", r'(?i)(?:wp-includes|wp-content)[^"\']*(?:\?|&)ver=([0-9]+(?:\.[0-9]+){0,2})'),
    # Drupal: drupal.js often carries core version as v= or ver=
    ("Drupal", r'(?i)(?:^|/)(?:core/)?misc/drupal\.js[^"\']*(?:\?|&)(?:v|ver)=([0-9]+(?:\.[0-9]+){0,2})'),
    # Joomla: occasionally includes version in media URIs as ?ver=
    ("Joomla", r'(?i)media/(?:system|jui|cms)/[^"\']*(?:\?|&)ver=([0-9]+(?:\.[0-9]+){0,2})'),
]

HEADER_GENERATOR_VERSION_PATS = [
    ("Drupal", r'(?i)\bdrupal\s*([0-9]+(?:\.[0-9]+){0,2})\b'),
    ("WordPress", r'(?i)\bwordpress\s*([0-9]+(?:\.[0-9]+){0,2})\b'),
    ("Squiz Matrix", r'(?i)\bsquiz\s*matrix\s*([0-9]+(?:\.[0-9]+){0,2})\b'),
]

# =============================== Static Tech Stack (tooling env) ===============================
TECH_STACK = {
    "Python": platform.python_version(),
    "requests": requests.__version__,
    "httpx": httpx.__version__,
    "dnspython": getattr(dns, "__version__", ""),
    "ssl/OpenSSL": ssl.OPENSSL_VERSION,
    "OS": platform.platform(),
}

# =============================== Utility and Evidence Logging ===============================
MASTER_EVIDENCE: List[Dict[str, str]] = []

def log_evidence(section: str, detail: str):
    try:
        MASTER_EVIDENCE.append({"section": section, "detail": detail})
    except Exception:
        pass
    try:
        # Also forward to Laravel logs if configured
        log_to_laravel(section, detail)
    except Exception:
        pass

def sanitize_domain(domain: str) -> str:
    import re
    from urllib.parse import urlparse
    s = str(domain or "").strip()
    if not s:
        return ""
    if re.match(r'^[a-zA-Z][a-zA-Z0-9+.\-]*://', s):
        p = urlparse(s)
    else:
        p = urlparse('//' + s)
    host = (p.hostname or "").strip().strip(".").lower()
    if not host:
        s2 = re.sub(r'^[a-zA-Z][a-zA-Z0-9+.\-]*://', '', s)
        host = s2.split('/')[0].split('?')[0].split('#')[0].strip().strip('.').lower()
    try:
        host = host.encode('idna').decode('ascii')
    except Exception:
        pass
    host = re.sub(r'[^a-z0-9.\-]', '', host)
    if not host or len(host) > 255 or '..' in host or host.startswith('.'):
        return ""
    return host

def dedupe_evidence(evidence: List[Dict[str, str]]) -> List[Dict[str, str]]:
    seen = set()
    out = []
    for ev in evidence or []:
        key = (ev.get("section","").strip(), (ev.get("detail","") or "").strip())
        if key in seen:
            continue
        seen.add(key)
        out.append(ev)
    return out

# =============================== HTML Parsing Helpers (lightweight) ===============================
def _extract_meta_generator(html: str) -> List[str]:
    items = []
    if not html:
        return items
    try:
        # Capture meta tags containing generator attr (any order of attributes)
        # We'll capture full tag then extract content attribute
        for m in re.finditer(r'(?is)<meta[^>]+(?:name|property)\s*=\s*["\']generator["\'][^>]*>', html):
            tag = m.group(0)
            cm = re.search(r'(?is)\bcontent\s*=\s*["\']([^"\']+)["\']', tag)
            if cm:
                val = cm.group(1).strip()
                if val:
                    items.append(val)
    except Exception:
        pass
    return items

def _extract_asset_urls(html: str) -> List[str]:
    urls = []
    if not html:
        return urls
    try:
        for m in re.finditer(r'(?is)<(?:script|link)[^>]+?(?:src|href)\s*=\s*["\']([^"\']+)["\']', html):
            u = m.group(1)
            if u:
                urls.append(u)
    except Exception:
        pass
    return urls[:1000]

def _detect_versions_from_generator_vals(vals: List[str]) -> List[Dict[str, str]]:
    out = []
    for v in vals or []:
        low = v.lower()
        for name, rx in META_GENERATOR_VERSION_PATS:
            m = re.search(rx, v, flags=re.I)
            if m:
                out.append({"component": name, "version": m.group(1), "source": "page:meta:generator", "observed": v})
            else:
                # If generator clearly indicates a CMS but no version, record presence
                if name.lower() in low:
                    out.append({"component": name, "version": "", "source": "page:meta:generator", "observed": v})
    return out

def _detect_versions_from_assets(urls: List[str]) -> List[Dict[str, str]]:
    out = []
    for u in urls or []:
        for name, rx in ASSET_VERSION_PATS:
            m = re.search(rx, u, flags=re.I)
            if m:
                out.append({"component": name, "version": m.group(1), "source": "page:asset", "observed": u})
            else:
                # Presence-only for CMS asset paths
                if name == "WordPress" and re.search(r'(?i)(wp-includes|wp-content)', u):
                    out.append({"component": "WordPress", "version": "", "source": "page:asset", "observed": u})
                if name == "Drupal" and re.search(r'(?i)(^|/)(core/)?misc/drupal\.js', u):
                    out.append({"component": "Drupal", "version": "", "source": "page:asset", "observed": u})
                if name == "Joomla" and re.search(r'(?i)\bjoomla\b', u):
                    out.append({"component": "Joomla", "version": "", "source": "page:asset", "observed": u})
    return out

def _detect_versions_from_headers(headers: Dict[str, str]) -> List[Dict[str, str]]:
    out = []
    if not headers:
        return out
    # X-Generator header is popular for Drupal and others
    gen = headers.get("x-generator") or headers.get("x-powered-by")
    if gen:
        for name, rx in HEADER_GENERATOR_VERSION_PATS:
            m = re.search(rx, gen, flags=re.I)
            if m:
                out.append({"component": name, "version": m.group(1), "source": "header:x-generator/x-powered-by", "observed": gen})
        # Presence without version
        gen_low = gen.lower()
        if "drupal" in gen_low and not any(x.get("component") == "Drupal" for x in out):
            out.append({"component": "Drupal", "version": "", "source": "header:x-generator", "observed": gen})
        if "wordpress" in gen_low and not any(x.get("component") == "WordPress" for x in out):
            out.append({"component": "WordPress", "version": "", "source": "header:x-generator", "observed": gen})
    # Drupal cache headers signal Drupal presence
    if headers.get("x-drupal-cache") or headers.get("x-drupal-dynamic-cache"):
        if not any(x.get("component") == "Drupal" for x in out):
            out.append({"component": "Drupal", "version": "", "source": "header:x-drupal-cache", "observed": f"x-drupal-cache={headers.get('x-drupal-cache','')}, x-drupal-dynamic-cache={headers.get('x-drupal-dynamic-cache','')}"})
    return out

def extract_page_component_versions(headers: Dict[str, str], html: str) -> List[Dict[str, str]]:
    """
    Primary page-first extractor. Detects CMS/framework presence and version from:
    - <meta name="generator" ...>
    - Asset URLs in <script src|link href> (e.g., drupal.js?v=10.2.5, wp-includes/... ?ver=6.5.2)
    - X-Generator and Drupal cache headers
    """
    meta_vals = _extract_meta_generator(html or "")
    urls = _extract_asset_urls(html or "")
    out = []
    out.extend(_detect_versions_from_generator_vals(meta_vals))
    out.extend(_detect_versions_from_assets(urls))
    out.extend(_detect_versions_from_headers(headers or {}))
    # Deduplicate while preserving any with versions first
    dedup = {}
    for it in out:
        key = (it["component"], it.get("version",""), it.get("source",""))
        if key in dedup:
            continue
        # prefer entries with version not empty
        nover_key = it["component"]
        if it.get("version"):
            # keep the first versioned entry for this component
            if nover_key not in dedup or (isinstance(dedup[nover_key], dict) and not dedup[nover_key].get("version")):
                dedup[nover_key] = it
        else:
            if nover_key not in dedup:
                dedup[nover_key] = it
    # Flatten
    final = []
    for v in dedup.values():
        if isinstance(v, dict):
            final.append(v)
    return final

def _apply_sigs(sigs: List[Tuple[str, str, float]], hay: str) -> List[Dict[str, Any]]:
    out = []
    for name, pattern, conf in sigs:
        try:
            if re.search(pattern, hay, flags=re.I):
                out.append({"name": name, "confidence": conf, "evidence": pattern})
        except re.error:
            # Fallback simple contains
            if pattern.lower() in hay:
                out.append({"name": name, "confidence": conf, "evidence": pattern})
    return out

def _haystack(headers: Dict[str, str], html: str, cookies: Dict[str, str]) -> str:
    h = " ".join([f"{k}: {v}" for k, v in headers.items()]).lower()
    c = " ".join([f"{k}={v}" for k, v in cookies.items()]).lower()
    txt = (html or "")[:HTML_MAX_BYTES].lower()
    return " ".join([h, c, txt])

def rich_stack_fingerprint(headers: Dict[str, str], html: str, cookies: Dict[str, str], dns_services: List[Dict[str, str]], page_versions: List[Dict[str, str]]) -> Dict[str, Any]:
    hay = _haystack(headers, html, cookies)
    cms = _apply_sigs(CMS_SIGS, hay)
    fw = _apply_sigs(FRAMEWORK_SIGS, hay)
    server = _apply_sigs(SERVER_SIGS, hay)
    cdn = _apply_sigs(CDN_SIGS, hay)
    lang = _apply_sigs(LANG_SIGS, hay)

    # Integrate page-first version signals to boost/confim CMS entries
    versions_map = {}  # component -> version
    for pv in page_versions or []:
        comp = pv.get("component")
        ver = pv.get("version") or ""
        if comp and ver and comp not in versions_map:
            versions_map[comp] = ver
        # If CMS component not yet in cms list, add as a low/no evidence detection
        if comp in ["Drupal", "WordPress", "Joomla"]:
            if not any(x["name"] == comp for x in cms):
                cms.append({"name": comp, "confidence": 0.9, "evidence": pv.get("source","page")})

    # Leverage DNS-inferred services to boost CDN/cloud confidence
    dns_names = " ".join([d.get("name","").lower() for d in dns_services or []])
    if "fastly" in dns_names and not any(x["name"] == "Fastly" for x in cdn):
        cdn.append({"name":"Fastly", "confidence":0.9, "evidence":"dns"})
    if "mimecast" in dns_names and not any(x["name"].lower().startswith("mimecast") for x in cdn):
        cdn.append({"name":"Mimecast (Email)", "confidence":0.85, "evidence":"dns"})

    def dedup(items):
        best = {}
        for it in items:
            name = it["name"]
            if name not in best or it["confidence"] > best[name]["confidence"]:
                best[name] = it
        return list(best.values())

    cms = dedup(cms)
    fw = dedup(fw)
    server = dedup(server)
    cdn = dedup(cdn)
    lang = dedup(lang)

    # Build a crisp summary (append versions if known)
    summary_parts = []
    if cms:
        cms_name = cms[0]["name"]
        if versions_map.get(cms_name):
            summary_parts.append(f"{cms_name} {versions_map[cms_name]}")
        else:
            summary_parts.append(cms_name)
    if fw:
        summary_parts.append(fw[0]["name"])
    if lang and (not fw or lang[0]["name"] not in fw[0]["name"]):
        summary_parts.append(lang[0]["name"])
    if server:
        summary_parts.append(server[0]["name"])
    if cdn:
        summary_parts.append("CDN: " + cdn[0]["name"])

    summary = " / ".join(summary_parts) if summary_parts else "Unknown"

    # Expose version signals
    versions_list = []
    for comp, ver in versions_map.items():
        versions_list.append({"component": comp, "version": ver, "source": "page"})

    return {
        "summary": summary,
        "cms": cms,
        "frameworks": fw,
        "server": server,
        "cdn": cdn,
        "languages": lang,
        "versions": versions_list
    }

# =============================== Nmap (optional) ===============================
def port_scan(ip: str, max_ports: int = 1000) -> List[Dict[str, Any]]:
    results = []
    try:
        import nmap
    except Exception as e:
        msg = f"python-nmap not available: {e}"
        log_evidence("Port Scan", msg)
        return [{"proto": '', "port": '', "service":'', "product":'', "version":'', "state":msg}]
    scan_args_su = f"-sS -sU --top-ports {max_ports} -T4"
    scan_args_tcp = f"-sT --top-ports {max_ports} -T4"
    nm = nmap.PortScanner()
    try:
        nm.scan(ip, arguments=scan_args_su)
        if ip not in nm.all_hosts() or not nm[ip].all_protocols():
            raise Exception("No protocols returned—scan probably failed.")
        for proto in nm[ip].all_protocols():
            for port in nm[ip][proto]:
                o = nm[ip][proto][port]
                results.append({
                    'proto': proto,
                    'port': port,
                    'service': o.get("name"),
                    'product': o.get("product", ""),
                    'version': o.get("version", ""),
                    'state': o.get("state", "") })
                log_evidence("Port Scan", f"{proto.upper()} {port}: {o.get('name')}, {o.get('product')}, {o.get('version')}, {o.get('state')}")
        return results
    except Exception as e:
        if "root privileges" in str(e).lower() or "quitting" in str(e).lower():
            try:
                nm.scan(ip, arguments=scan_args_tcp)
                if ip in nm.all_hosts():
                    for proto in nm[ip].all_protocols():
                        for port in nm[ip][proto]:
                            o = nm[ip][proto][port]
                            results.append({
                                'proto': proto,
                                'port': port,
                                'service': o.get("name"),
                                'product': o.get("product", ""),
                                'version': o.get("version", ""),
                                'state': o.get("state", "") })
                            log_evidence("Port Scan", f"{proto.upper()} {port}: {o.get('name')}, {o.get('product')}, {o.get('version')}, {o.get('state')}")
                return results if results else [{"proto": '', "port": '', "service":'', "product":'', "version":'', "state":"No open TCP ports found (TCP connect scan)"}]
            except Exception as ex2:
                log_evidence("Port Scan", f"nmap error: {ex2}")
                return [{"proto": '', "port": '', "service":'', "product":'', "version":'', "state":f"nmap error: {ex2}"}]
        else:
            log_evidence("Port Scan", f"nmap error: {e}")
            return [{"proto": '', "port": '', "service":'', "product":'', "version":'', "state":f"nmap error: {e}"}]

# =============================== Reconnaissance ===============================
def recon_subdomains(domain: str) -> List[str]:
    url = f"https://crt.sh/?q=%25.{domain}&output=json"
    try:
        r = requests.get(url, timeout=10)
        entries = r.json()
        subdomains = set()
        for x in entries:
            if "name_value" in x:
                for s in x["name_value"].split("\n"):
                    if s.strip().endswith(domain):
                        subdomains.add(s.strip())
        log_evidence("Subdomain Recon", f"Found: {', '.join(list(subdomains)[:10])}")
        return sorted(list(subdomains))
    except Exception as e:
        log_evidence("Subdomain Recon", f"Error: {e}")
        return []

def recon_dns(domain: str) -> Dict[str, Any]:
    ips, evidence = [], []
    res: Dict[str, Any] = {"ips": [], "evidence": evidence}
    try:
        answer = dns.resolver.resolve(domain, 'A', lifetime=5)
        ips = [str(r) for r in answer]
        res["ips"] = ips
        log_evidence("DNS Recon", f"A records: {ips}")
    except Exception as ex:
        evidence.append(f"Could not resolve domain: {ex}")
        log_evidence("DNS Recon", evidence[-1])
    for rec in ['PTR', 'MX', 'TXT']:
        try:
            if rec == "PTR" and ips:
                ptr = []
                for ip in ips:
                    try:
                        ptr_name = str(dns.reversename.from_address(ip))
                        ptr.append(ptr_name)
                    except Exception: pass
                res['ptr'] = ptr
                if ptr: log_evidence("DNS Recon", f"PTR: {ptr}")
            elif rec == "MX":
                mx = [str(r.exchange).rstrip('.') for r in dns.resolver.resolve(domain, 'MX', lifetime=5)]
                res['mx'] = mx
                if mx: log_evidence("DNS Recon", f"MX: {mx}")
            elif rec == "TXT":
                txt = [bytes(r.strings[0]).decode('utf-8', errors='ignore') if getattr(r, 'strings', None) else str(r) for r in dns.resolver.resolve(domain, 'TXT', lifetime=5)]
                spf = [t for t in txt if "v=spf1" in t]
                dmarc = []
                try:
                    dmarc_txt = [str(x.strings[0], 'utf-8') for x in dns.resolver.resolve(f"_dmarc.{domain}", 'TXT', lifetime=5)]
                    dmarc.extend([t for t in dmarc_txt if "v=DMARC1" in t])
                except Exception:
                    pass
                res['txt'] = txt
                res['spf'] = spf
                res['dmarc'] = dmarc
                if txt: log_evidence("DNS Recon", f"TXT: {txt[:3]}{'...' if len(txt)>3 else ''}")
        except Exception:
            continue
    return res

def recon_asn(domain: str) -> Dict[str, Any]:
    try:
        ips = recon_dns(domain)["ips"]
        asns = []
        for ip in ips:
            r = requests.get(f"https://ipinfo.io/{ip}/json", timeout=8)
            if r.ok:
                data = r.json()
                asns.append(data.get("org", "unknown"))
        log_evidence("ASN Recon", f"ASNs: {asns}")
        return {"asns": asns}
    except Exception as e:
        log_evidence("ASN Recon", f"Error: {e}")
        return {"asns": []}

# =============================== TLS Probe ===============================
def tls_probe(domain: str, port: int = 443) -> Dict[str, Any]:
    import ssl
    import socket
    from datetime import datetime, timezone

    def _log(msg: str):
        try:
            log_evidence("TLS Probe", msg)
        except Exception:
            try:
                print(f"TLS Probe: {msg}")
            except Exception:
                pass

    def _format_cert_name(name) -> str:
        parts = []
        try:
            for rdn in name or []:
                for attr in rdn:
                    if isinstance(attr, tuple):
                        if len(attr) >= 2:
                            k, v = attr[0], attr[1]
                            if isinstance(k, bytes):
                                k = k.decode("utf-8", errors="ignore")
                            if isinstance(v, bytes):
                                v = v.decode("utf-8", errors="ignore")
                            parts.append(f"{k}={v}")
                        else:
                            parts.append("=".join(str(x) for x in attr))
                    else:
                        parts.append(str(attr))
        except Exception:
            try:
                parts.append(str(name))
            except Exception:
                pass
        return ", ".join(parts)

    def _probe_with_context(ctx: ssl.SSLContext) -> dict:
        with socket.create_connection((domain, port), timeout=10) as sock:
            with ctx.wrap_socket(sock, server_hostname=domain) as ssock:
                ver = ssock.version() or ""
                ciph = ssock.cipher()
                cipher = ciph[0] if isinstance(ciph, tuple) and len(ciph) > 0 else (ciph or "")
                cert = ssock.getpeercert() or {}

                not_after = cert.get("notAfter") or ""
                issuer = _format_cert_name(cert.get("issuer"))
                subject = _format_cert_name(cert.get("subject"))

                days_left = ""
                if not_after:
                    try:
                        dt = datetime.strptime(not_after, "%b %d %H:%M:%S %Y %Z").replace(tzinfo=timezone.utc)
                        days_left = (dt - datetime.now(timezone.utc)).days
                    except Exception:
                        days_left = ""

                info = {
                    "status": "ok",
                    "version": ver,
                    "cipher": cipher,
                    "cert_not_after": not_after,
                    "cert_days_remaining": days_left,
                    "issuer": issuer,
                    "subject": subject,
                }
                _log(f"TLS {ver}, cipher {cipher}, cert expires {not_after}, days_left={days_left}")
                return info

    try:
        try:
            ctx = ssl.create_default_context()
            return _probe_with_context(ctx)
        except ssl.SSLError:
            try:
                ctx2 = ssl.create_default_context()
                ctx2.check_hostname = False
                ctx2.verify_mode = ssl.CERT_NONE
                return _probe_with_context(ctx2)
            except ssl.SSLError as e2:
                _log(f"TLS verification/handshake failed: {e2}")
                return {"status": f"unavailable", "error": f"{e2}"}
    except Exception as e:
        _log(f"TLS error: {e}")
        return {"status": f"error: {e}"}

# =============================== LLM Fuzzy Tech Guess (constrained) ===============================
def llm_guess_tech(headers: Dict[str, str], html: str, previous_summary: str = "") -> str:
    # Constrain output to single-line "Stack: ..."
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": PROMPT_TECH_SYSTEM},
                {"role": "user", "content": PROMPT_TECH_USER_TEMPLATE.format(
                    headers=json.dumps(headers)[:4000],
                    html=(html or "")[:800],
                    prev=(previous_summary or "")
                )}
            ],
            temperature=0.0,
        )
        content = (resp.choices[0].message.content or "").strip()
        m = re.search(r'(?i)^stack:\s*(.+)$', content.strip(), flags=re.M)
        if m:
            return "Stack: " + m.group(1).strip()
        cleaned = re.sub(r'[\r\n]+', ' ', content)
        if "Stack:" not in cleaned:
            cleaned = "Stack: " + cleaned
        return cleaned[:200]
    except Exception as e:
        log_evidence("LLM Stack Reasoning", f"Model error: {e}")
        return "Stack: Unknown"

# =============================== DNS-based tech inference ===============================
DNS_TECH_PATTERNS = [
    (r'fastly-domain-delegation', 'Fastly (CDN/Edge)', 'TXT'),
    (r'\bmimecast\b|_netblocks\.mimecast\.com', 'Mimecast (Email Security)', 'MX/SPF/TXT'),
    (r'amazonses', 'Amazon SES (Email Delivery)', 'SPF/TXT'),
    (r'_spf\.createsend\.com', 'Campaign Monitor/CreateSend (Email)', 'SPF'),
    (r'_spf\.mlsend\.com|mlsend', 'MailerLite (Email)', 'SPF/TXT'),
    (r'ap\.rp\.oracleemaildelivery\.com', 'Oracle Cloud Email Delivery', 'SPF'),
    (r'mandrill', 'Mandrill/Mailchimp (Email)', 'TXT'),
    (r'atlassian-domain-verification', 'Atlassian Cloud', 'TXT'),
    (r'google-site-verification', 'Google Search Console', 'TXT'),
    (r'apple-domain-verification', 'Apple Services', 'TXT'),
    (r'smartsheet-site-validation', 'Smartsheet', 'TXT'),
    (r'miro-verification', 'Miro', 'TXT'),
    (r'mentimeter', 'Mentimeter', 'TXT'),
    (r'teamviewer-sso-verification', 'TeamViewer SSO', 'TXT'),
    (r'intersight', 'Cisco Intersight', 'TXT'),
    (r'quovadis', 'QuoVadis (CA/PKI)', 'TXT'),
    (r'nearmap', 'Nearmap', 'TXT'),
]

def infer_providers_from_dns(dnsres: Dict[str, Any]) -> List[Dict[str, str]]:
    items = []
    try:
        txt_all = " ".join(dnsres.get("txt", []) or [])
        spf_all = " ".join(dnsres.get("spf", []) or [])
        mx_all = " ".join(dnsres.get("mx", []) or [])
        haystack = " ".join([txt_all, spf_all, mx_all]).lower()
        for rx, name, source in DNS_TECH_PATTERNS:
            if re.search(rx, haystack):
                items.append({"name": name, "source": source, "pattern": rx})
        if items:
            log_evidence("DNS Tech Inference", "Providers: " + ", ".join([i["name"] for i in items]))
    except Exception as e:
        log_evidence("DNS Tech Inference", f"Error: {e}")
    return items

# =============================== HTTP Probe (smarter fetch + redirects) ===============================
def _make_http_client(follow_redirects: bool = True) -> httpx.Client:
    headers = {
        "User-Agent": random.choice(USER_AGENT_POOL),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "close"
    }
    return httpx.Client(
        headers=headers,
        verify=False,  # we probe only; TLS assessment is separate
        timeout=HTTP_TIMEOUT,
        follow_redirects=follow_redirects,
        limits=httpx.Limits(max_keepalive_connections=0, max_connections=5)
    )

def _fetch_url(client_http: httpx.Client, url: str) -> Tuple[Any, str]:
    try:
        resp = client_http.get(url)
        return resp, ""
    except Exception as e:
        return None, str(e)

def _consolidate_html(*parts: str) -> str:
    buf = []
    total = 0
    for p in parts:
        if not p:
            continue
        chunk = p[: max(0, HTML_MAX_BYTES - total)]
        if not chunk:
            break
        buf.append(chunk)
        total += len(chunk)
        if total >= HTML_MAX_BYTES:
            break
    return "".join(buf)

def _safe_small_get(client_http: httpx.Client, url: str) -> Tuple[str, int]:
    try:
        r = client_http.get(url, headers={"Range": f"bytes=0-{MAX_PROBE_BYTES}"})
        if r.status_code < 400 and ("text" in (r.headers.get("content-type","").lower()) or "xml" in (r.headers.get("content-type","").lower()) or "html" in (r.headers.get("content-type","").lower())):
            txt = r.text[:MAX_PROBE_BYTES]
            return txt, r.status_code
        return "", r.status_code
    except Exception:
        return "", 0

def _probe_cms_versions_safe(client_http: httpx.Client, base_url: str) -> List[Dict[str, str]]:
    if not SAFE_VERSION_PROBES or not base_url:
        return []
    found = []
    try:
        # Normalize base
        pu = urlparse(base_url)
        root = f"{pu.scheme}://{pu.netloc}"
    except Exception:
        root = base_url
    # Drupal CHANGELOG
    for comp, paths in VERSION_PROBE_PATHS.items():
        for p in paths:
            try:
                url = urljoin(root, p)
                body, code = _safe_small_get(client_http, url)
                if not body or code >= 400:
                    continue
                if comp == "Drupal":
                    m = re.search(r'(?im)^\s*Drupal\s+([0-9]+\.[0-9]+(?:\.[0-9]+)?)\b', body)
                    if m:
                        ver = m.group(1)
                        found.append({"component": "Drupal", "version": ver, "source": f"probe:{p}", "observed": f"{p}: Drupal {ver}"})
                elif comp == "WordPress":
                    m = re.search(r'(?im)\bVersion\s+([0-9]+(?:\.[0-9]+){0,2})\b', body)
                    if m:
                        ver = m.group(1)
                        found.append({"component": "WordPress", "version": ver, "source": f"probe:{p}", "observed": f"{p}: Version {ver}"})
                elif comp == "Joomla":
                    m = re.search(r'(?is)<version>\s*([0-9]+(?:\.[0-9]+){0,2})\s*</version>', body)
                    if m:
                        ver = m.group(1)
                        found.append({"component": "Joomla", "version": ver, "source": f"probe:{p}", "observed": f"{p}: {ver}"})
            except Exception:
                continue
    if found:
        log_evidence("Safe Version Probe", ", ".join([f"{x['component']} {x['version']} via {x['source']}" for x in found]))
    return found

def probe_http(domain: str) -> Dict[str, Any]:
    headers: Dict[str, str] = {}
    cookies: Dict[str, str] = {}
    html = ""
    set_cookies_raw: List[str] = []
    status_code = None
    redirects: List[str] = []
    final_url = ""
    status_chain: List[int] = []
    evidence: List[str] = []
    waf_flag = None

    tried_urls = []
    candidates = [
        f"https://{domain}",
        f"http://{domain}",
    ]
    # Try common www. alternates if not explicitly using www.
    if not domain.lower().startswith("www."):
        candidates.extend([f"https://www.{domain}", f"http://www.{domain}"])

    best_resp = None
    best_err = ""
    page_versions = []

    for idx, base_url in enumerate(candidates):
        tried_urls.append(base_url)
        with _make_http_client(follow_redirects=True) as client_http:
            resp, err = _fetch_url(client_http, base_url)
            if resp is not None:
                # Record status chain
                hist = getattr(resp, "history", [])
                redirects = [str(h.url) for h in hist]
                status_chain = [h.status_code for h in hist] + [resp.status_code]
                status_code = resp.status_code
                final_url = str(resp.url)
                headers = {k.lower(): str(v) for k, v in resp.headers.items()}
                cookies = dict(resp.cookies)
                # Set-Cookie lines
                try:
                    set_cookies_raw = resp.headers.get_list("set-cookie")
                except Exception:
                    if 'set-cookie' in headers:
                        set_cookies_raw = [headers['set-cookie']]
                # Body (cap length)
                html = (resp.text or "")[:HTML_MAX_BYTES]
                evidence.append(f"GET {base_url} => {resp.status_code} (final: {final_url})")

                # Page-first stack version extraction from landing page
                try:
                    page_versions = extract_page_component_versions(headers, html)
                except Exception:
                    page_versions = []

                # Also probe HEAD and OPTIONS on the final URL to enrich headers
                try:
                    resp_head = client_http.head(final_url)
                    evidence.append(f"HEAD {final_url}: {resp_head.status_code}")
                    for k, v in resp_head.headers.items():
                        kl = k.lower()
                        if kl not in headers:
                            headers[kl] = str(v)
                    # Re-check versions after HEAD merged headers (x-generator etc.)
                    try:
                        extra_page_versions = _detect_versions_from_headers({k.lower(): v for k, v in resp_head.headers.items()})
                        # merge
                        page_versions += [x for x in extra_page_versions if x not in page_versions]
                    except Exception:
                        pass
                except Exception:
                    pass
                try:
                    resp_opts = client_http.options(final_url)
                    evidence.append(f"OPTIONS {final_url}: Allow: {resp_opts.headers.get('allow','')}")
                    for k, v in resp_opts.headers.items():
                        kl = k.lower()
                        if kl not in headers:
                            headers[kl] = str(v)
                except Exception:
                    pass

                # If we landed on a non-HTML page with small/no content, try robots/sitemap to glean tech
                addl_html = ""
                for path in ["/robots.txt", "/sitemap.xml"]:
                    try:
                        u = urljoin(final_url, path)
                        r2 = client_http.get(u)
                        if r2 and r2.status_code < 400 and r2.headers.get("content-type","").lower().startswith(("text","application/xml")):
                            addl_html += "\n" + (r2.text or "")[:20000]
                    except Exception:
                        pass
                if addl_html:
                    html = _consolidate_html(html, addl_html)

                # Safe, non-intrusive CMS version probes (CHANGELOG/readme)
                safe_versions = _probe_cms_versions_safe(client_http, final_url)
                if safe_versions:
                    page_versions += [x for x in safe_versions if x not in page_versions]

                best_resp = resp
                break
            else:
                evidence.append(f"Fetch error for {base_url}: {err}")
                best_err = err

    if best_resp is None:
        # Give up
        log_evidence("HTTP Probe", f"All fetch attempts failed for {domain}. Last error: {best_err}")
        return {
            "error": best_err,
            "status": "-",
            "headers": {},
            "redirects": [],
            "tls": {"status": "unavailable"},
            "waf": None,
            "login_panel": False,
            "cookies": {},
            "evidence": evidence,
            "frameworks": "Unknown",
            "html_sample": "",
            "html_full": "",
            "set_cookies_raw": [],
            "status_chain": [],
            "final_url": "",
            "detected_versions": []
        }

    # Log raw probe info
    content_len = len((html or "").encode("utf-8"))
    evidence.append(f"Body Content Size: {content_len} bytes")
    log_evidence("HTTP Probe", f"{evidence} | Headers: {json.dumps(headers)[:4000]} | Chain: {status_chain}")

    # TLS details
    tls = tls_probe(domain, 443)

    # WAF/banner heuristics
    waf_flag = any(
        any(x in (v or "").lower() for x in ["cloudflare", "akamai", "imperva", "incapsula", "sucuri", "fastly"])
        for _, v in headers.items()
    )

    # Detect login
    login_hint = False
    try:
        if html and re.search(r'password|<input[^>]+type=["\']password', html, flags=re.I):
            login_hint = True
    except Exception:
        pass

    # Enhanced stack fingerprint (heuristic-first) with page-first versions
    stack_report = rich_stack_fingerprint(headers, html or "", cookies, [], page_versions or [])
    frameworks_summary = stack_report.get("summary") or "Unknown"

    # Fall back to basic detector or LLM if unknown/low-info
    if frameworks_summary == "Unknown":
        fw_basic = detect_frameworks_basic(headers, cookies)
        if fw_basic != "Unknown":
            frameworks_summary = fw_basic
        else:
            llm_stack = llm_guess_tech(headers, html or "", previous_summary="")
            frameworks_summary = re.sub(r'(?i)^stack:\s*', '', llm_stack).strip()
            if not frameworks_summary:
                frameworks_summary = "Unknown"

    return dict(
        status=status_code if status_code is not None else "-",
        headers=headers,
        redirects=redirects,
        tls=tls,
        waf=waf_flag,
        login_panel=login_hint,
        cookies=cookies,
        evidence=evidence,
        frameworks=frameworks_summary,
        html_sample=(html or "")[:2000],
        html_full=(html or "")[:HTML_MAX_BYTES],
        set_cookies_raw=set_cookies_raw,
        status_chain=status_chain,
        final_url=final_url,
        stack_report=stack_report,
        detected_versions=page_versions
    )

# =============================== Vulnerability Mapping ===============================
def lookup_vulns(tech: str, version: str = "") -> List[Dict[str, str]]:
    out = []
    try:
        q = f"{tech} {version}".strip()
        r = requests.get(f"https://cve.circl.lu/api/search/{q}", timeout=10)
        data = r.json()
        for x in data.get("results", []):
            out.append({"cve": x.get("id", ""), "summary": x.get("summary", ""), "source": "cve.circl.lu"})
    except Exception:
        pass
    return out

def get_recommendation(finding: Dict[str, Any]) -> str:
    title = (finding.get('title') or "").lower()
    if title.startswith("open port"):
        return f"Restrict unnecessary exposure of port {finding.get('port')}. Enforce firewall rules and service ACLs; close or limit to trusted sources."
    if 'ssl' in title or 'tls' in title:
        return "Enforce TLS 1.2+ (prefer 1.3), disable weak ciphers, use modern curves, and maintain a valid certificate with short lifetime."
    if 'outdated' in title or 'cve' in title or 'unsupported' in title or 'eol' in title:
        return "Upgrade to a supported version, apply security patches, and monitor vendor advisories."
    if 'cookie' in title:
        return "Set Secure, HttpOnly, and SameSite on cookies; minimize sensitive data in cookies."
    if 'header' in title:
        return "Set HSTS, CSP, X-Frame-Options, X-Content-Type-Options, Referrer-Policy, and Permissions-Policy per best practice."
    if 'missing spf' in title or 'missing dmarc' in title:
        return "Publish SPF/DMARC records and enforce policy to reduce spoofing."
    return "Review and mitigate per referenced CVE or best practices."

# =============================== LLM Utilities ===============================
def llm_instruct(context: str, goal: str) -> str:
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": PROMPT_LLMI_INSTRUCT_SYSTEM},
                {"role": "user", "content": PROMPT_LLMI_INSTRUCT_USER_TEMPLATE.format(context=context[:6000], goal=goal)}
            ],
            temperature=0.2
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        log_evidence("LLM", f"LLM error: {e}")
        return f""

LLM_CACHE: Dict[str, Dict[str, Any]] = {}
def _safe_json_loads(s: str) -> Any:
    try:
        return json.loads(s)
    except Exception:
        m = re.search(r'\{.*\}', s, re.S)
        if m:
            try:
                return json.loads(m.group(0))
            except Exception:
                return None
        return None

def llm_support_vuln_check(component: str, version: str, context: str = "") -> Dict[str, Any]:
    key = f"{component.lower()}::{version}"
    if key in LLM_CACHE:
        return LLM_CACHE[key]
    try:
        resp = client.chat.completions.create(
            model="gpt-4.1",
            messages=[
                {"role": "system", "content": PROMPT_SUPPORT_VULN_SYSTEM},
                {"role": "user", "content": PROMPT_SUPPORT_VULN_USER_TEMPLATE.format(
                    today=datetime.utcnow().date().isoformat(),
                    component=component, version=version, context=context[:500]
                )}
            ],
            temperature=0.0
        )
        content = (resp.choices[0].message.content or "").strip()
        data = _safe_json_loads(content) or {
            "component": component, "version": version,
            "support_status": "Unknown", "known_vulnerabilities": [], "notes": ""
        }
    except Exception as e:
        log_evidence("LLM Support/Vuln", f"Error: {e} for {component} {version}")
        data = {"component": component, "version": version, "support_status": "Unknown", "known_vulnerabilities": [], "notes": ""}

    data.setdefault("component", component)
    data.setdefault("version", version)
    data.setdefault("support_status", "Unknown")
    kv = data.get("known_vulnerabilities", [])
    if not isinstance(kv, list):
        kv = []
    for v in kv:
        if isinstance(v, dict) and "summary" in v and isinstance(v["summary"], str):
            v["summary"] = (v["summary"][:160] + "...") if len(v["summary"]) > 160 else v["summary"]
    data["known_vulnerabilities"] = kv
    data["notes"] = (data.get("notes") or "")[:180]
    LLM_CACHE[key] = data
    cves = ", ".join([v.get("id", "") for v in kv if isinstance(v, dict) and v.get("id")])
    log_evidence("LLM Support/Vuln", f"{component} {version} => {data.get('support_status')} | CVEs: {cves} | Notes: {data.get('notes')}")
    return data

# =============================== Version Extraction Helpers ===============================
def extract_name_version_pair(text: str) -> Tuple[str, str]:
    # Enhanced to allow major-only versions (e.g., "Drupal 10")
    patterns = [
        r'(?i)\b([A-Za-z][A-Za-z0-9\-\.\+ ]{1,40})/(v?\d+(?:\.\d+)*)',
        r'(?i)\b([A-Za-z][A-Za-z0-9\-\.\+ ]{1,40})\s+(v?\d+(?:\.\d+)*)',
        r'(?i)\b([A-Za-z][A-Za-z0-9\-\.\+ ]{1,40})-(v?\d+(?:\.\d+)*)',
    ]
    for pat in patterns:
        m = re.search(pat, text or "")
        if m:
            name = m.group(1).strip()
            ver = m.group(2).strip().lstrip('vV')
            return name, ver
    m2 = re.search(r'(?i)\b(v?\d+(?:\.\d+)*)\b', text or "")
    if m2:
        return (text.strip(), m2.group(1).lstrip('vV'))
    return ("", "")

def versioned_entries_from_headers(headers: Dict[str, str]) -> List[Dict[str, str]]:
    entries = []
    for hk, hv in headers.items():
        name, ver = extract_name_version_pair(hv)
        if ver:
            entries.append({"component": name, "version": ver, "source": f"header:{hk}", "observed": hv})
        else:
            # Special-case X-Generator presence without numeric minor (e.g., "Drupal 10")
            if hk.lower() in ("x-generator","x-powered-by","server"):
                for cname in ["Drupal", "WordPress", "Joomla", "Squiz Matrix"]:
                    if cname.lower() in hv.lower():
                        entries.append({"component": cname, "version": "", "source": f"header:{hk}", "observed": hv})
                        break
    return entries

def versioned_entries_from_ports(ports: List[Dict[str, Any]]) -> List[Dict[str, str]]:
    entries = []
    for p in ports:
        banner = " ".join([str(p.get("product") or "").strip(), str(p.get("version") or "").strip()]).strip()
        if not banner:
            banner = " ".join([str(p.get("service") or "").strip(), str(p.get("version") or "").strip()]).strip()
        if banner:
            name, ver = extract_name_version_pair(banner)
            if ver:
                entries.append({
                    "component": name or (p.get("product") or p.get("service") or "service"),
                    "version": ver,
                    "source": f"port:{p.get('port')}/{p.get('proto')}",
                    "observed": banner
                })
    return entries

def versioned_entries_from_page(http_res: Dict[str, Any]) -> List[Dict[str, str]]:
    out = []
    for it in (http_res.get("detected_versions") or []):
        out.append({
            "component": it.get("component",""),
            "version": it.get("version",""),
            "source": it.get("source","page"),
            "observed": it.get("observed","")
        })
    # Also parse generator meta and assets from full HTML if not already captured
    headers = http_res.get("headers", {}) or {}
    html = http_res.get("html_full") or http_res.get("html_sample") or ""
    more = extract_page_component_versions(headers, html)
    for it in more:
        row = {"component": it.get("component",""), "version": it.get("version",""), "source": it.get("source","page"), "observed": it.get("observed","")}
        if row not in out:
            out.append(row)
    return out

# =============================== Header & Cookie Analysis ===============================
def analyze_security_headers(headers: Dict[str, str]) -> Dict[str, Any]:
    h = {k.lower(): v for k, v in headers.items()}
    evaluated = bool(h)
    results = {
        "strict-transport-security": bool(h.get("strict-transport-security")),
        "content-security-policy": bool(h.get("content-security-policy")),
        "x-frame-options": bool(h.get("x-frame-options")),
        "x-content-type-options": bool(h.get("x-content-type-options")),
        "referrer-policy": bool(h.get("referrer-policy")),
        "permissions-policy": bool(h.get("permissions-policy"))
    }
    missing = [k for k, present in results.items() if not present] if evaluated else []
    if evaluated:
        log_evidence("Header Analysis", f"Missing headers: {missing}")
    else:
        log_evidence("Header Analysis", "No HTTP response to evaluate headers.")
    return {"present": results, "missing": missing, "evaluated": evaluated}

def parse_set_cookie_lines(set_cookie_raw: List[str]) -> List[Dict[str, Any]]:
    cookies = []
    for line in set_cookie_raw or []:
        try:
            parts = [p.strip() for p in line.split(";")]
            if not parts:
                continue
            name_val = parts[0]
            name = name_val.split("=")[0].strip()
            flags = {"Secure": False, "HttpOnly": False, "SameSite": None}
            for p in parts[1:]:
                if p.lower() == "secure":
                    flags["Secure"] = True
                elif p.lower() == "httponly":
                    flags["HttpOnly"] = True
                elif p.lower().startswith("samesite"):
                    ss = p.split("=", 1)[-1].strip()
                    flags["SameSite"] = ss
            cookies.append({"name": name, "flags": flags, "raw": line})
        except Exception:
            cookies.append({"name": "(parse_error)", "flags": {"Secure": False, "HttpOnly": False, "SameSite": None}, "raw": line})
    return cookies

def analyze_cookie_flags(set_cookie_raw: List[str]) -> Dict[str, Any]:
    parsed = parse_set_cookie_lines(set_cookie_raw)
    issues = []
    for c in parsed:
        missing = []
        if not c["flags"].get("Secure"):
            missing.append("Secure")
        if not c["flags"].get("HttpOnly"):
            missing.append("HttpOnly")
        if not c["flags"].get("SameSite"):
            missing.append("SameSite")
        if missing:
            issues.append({"cookie": c["name"], "missing": missing, "raw": c["raw"]})
    log_evidence("Cookie Analysis", f"Insecure cookies: {[i['cookie'] for i in issues]}")
    return {"parsed": parsed, "issues": issues}

# =============================== Findings Classification ===============================
def guess_section_type(finding: Dict[str, Any]) -> str:
    t = (finding.get("title","") + " " + finding.get("desc","") + " " + finding.get("evidence","")).lower()
    if "cross-site scripting" in t or "xss" in t:
        return "xss"
    if "upload" in t and "insecure" in t:
        return "upload"
    if "multi-factor" in t or "mfa" in t:
        return "lack_of_mfa"
    if "weak password" in t or "password policy" in t:
        return "password"
    if "session" in t:
        return "session"
    if ("httponly" in t or "secure" in t) and "cookie" in t:
        return "cookie"
    if "outdated" in t or "old version" in t or "cve" in t or "unsupported" in t or "eol" in t:
        return "outdated"
    if "header" in t or "content-security" in t or "x-frame" in t or "transport-security" in t:
        return "header"
    if "ssl" in t or "tls" in t:
        return "ssl"
    if re.search(r'\bversion\b.*\d+\.\d+', t) or re.search(r'/\d+\.\d+', t):
        return "version_info"
    if "spf" in t or "dmarc" in t or "dns record" in t:
        return "dns"
    return "other"

# =============================== Table of Contents Structure ===============================
TOC = [
    {"num":"1", "title":"Management Summary", "tag":None, "children":[
        {"num":"1.1", "title":"Background", "tag":TAG_ASSISTED},
        {"num":"1.2", "title":"Objectives and Scope", "tag":TAG_MANUAL},
        {"num":"1.3", "title":"Approach Overview (Automated + LLM-Assisted)", "tag":TAG_ASSISTED},
        {"num":"1.4", "title":"Key Results and Recommended Remediations", "tag":TAG_ASSISTED},
        {"num":"1.5", "title":"Risk Posture and Business Impact", "tag":TAG_ASSISTED},
        {"num":"1.6", "title":"Root Cause Analysis", "tag":TAG_ASSISTED},
        {"num":"1.7", "title":"Next Steps and Roadmap", "tag":TAG_ASSISTED},
    ]},
    {"num":"2", "title":"Engagement Overview and Rules of Engagement", "tag":None, "children":[
        {"num":"2.1", "title":"In-Scope and Out-of-Scope Assets", "tag":TAG_MANUAL},
        {"num":"2.2", "title":"Assumptions and Constraints", "tag":TAG_MANUAL},
        {"num":"2.3", "title":"Testing Windows, Contacts, and Communications", "tag":TAG_MANUAL},
        {"num":"2.4", "title":"Legal, Ethics, and Safe-Testing Guidelines", "tag":TAG_MANUAL},
    ]},
    {"num":"3", "title":"Methodology", "tag":None, "children":[
        {"num":"3.1", "title":"Automated Reconnaissance and Scanning Pipeline", "tag":TAG_AUTOMATABLE},
        {"num":"3.2", "title":"LLM-Assisted Analysis and Enrichment", "tag":TAG_ASSISTED, "children":[
            {"num":"3.2.1", "title":"Version Support and Vulnerability Assessment via LLM", "tag":TAG_AUTOMATABLE},
            {"num":"3.2.2", "title":"Prompt Design, Guardrails, and Hallucination Mitigation", "tag":TAG_MANUAL},
            {"num":"3.2.3", "title":"Human-in-the-Loop Validation and Triage", "tag":TAG_MANUAL},
        ]},
        {"num":"3.3", "title":"Manual Verification and Exploitation Procedures", "tag":TAG_MANUAL},
        {"num":"3.4", "title":"Severity Rating and Risk Scoring (e.g., CVSS)", "tag":TAG_ASSISTED},
        {"num":"3.5", "title":"Evidence Collection, Reproducibility, and Audit Trail", "tag":TAG_AUTOMATABLE},
        {"num":"3.6", "title":"Data Protection and Privacy (PII Handling and Redaction)", "tag":TAG_ASSISTED},
        {"num":"3.7", "title":"Limitations of Automated and LLM-Assisted Testing", "tag":TAG_ASSISTED},
    ]},
    {"num":"4", "title":"Technical Summary", "tag":None, "children":[
        {"num":"4.1", "title":"Target Overview and Attack Surface", "tag":TAG_ASSISTED},
        {"num":"4.2", "title":"Technology Stack and Versions (with LLM Support Status and Known CVEs)", "tag":TAG_AUTOMATABLE},
        {"num":"4.3", "title":"Network Summary (Hosts, Open Ports, Services, Products)", "tag":TAG_AUTOMATABLE},
        {"num":"4.4", "title":"Application Summary (Headers, TLS, WAF, Authentication)", "tag":TAG_ASSISTED},
        {"num":"4.5", "title":"DNS and Email Security Posture (SPF, DKIM, DMARC)", "tag":TAG_ASSISTED},
        {"num":"4.6", "title":"High-Risk Findings Summary and Quick Wins", "tag":TAG_ASSISTED},
    ]},
    {"num":"5", "title":"Detailed Findings – Penetration Test", "tag":None, "children":[
        {"num":"5.6", "title":"Insecure Cookie Parameters", "tag":TAG_AUTOMATABLE},
        {"num":"5.7", "title":"Outdated and Vulnerable Software in Use", "tag":TAG_ASSISTED},
        {"num":"5.8", "title":"Insecure HTTP Response Header Configuration", "tag":TAG_AUTOMATABLE},
        {"num":"5.9", "title":"Insecure SSL/TLS Configuration", "tag":TAG_AUTOMATABLE},
        {"num":"5.14", "title":"Version Information Disclosure", "tag":TAG_AUTOMATABLE},
        {"num":"5.15", "title":"Missing Security DNS Records", "tag":TAG_AUTOMATABLE},
        {"num":"5.X", "title":"Other Categories (Manual/Assisted stubs)", "tag":TAG_ASSISTED},
    ]},
    {"num":"6", "title":"Remediation Plan and Roadmap", "tag":None, "children":[
        {"num":"6.1", "title":"Prioritized Fix List (30/60/90 Days)", "tag":TAG_ASSISTED},
        {"num":"6.2", "title":"Dependency and Patch Management Strategy", "tag":TAG_ASSISTED},
        {"num":"6.3", "title":"Hardening Standards and Secure Configuration Baselines", "tag":TAG_ASSISTED},
        {"num":"6.4", "title":"SDLC and DevSecOps Integration", "tag":TAG_ASSISTED},
        {"num":"6.5", "title":"Monitoring, Detection, and Preventive Controls", "tag":TAG_ASSISTED},
    ]},
    {"num":"7", "title":"Compliance and Framework Mapping", "tag":None, "children":[
        {"num":"7.1", "title":"OWASP ASVS / Top 10", "tag":TAG_ASSISTED},
        {"num":"7.2", "title":"NIST CSF / NIST 800-53", "tag":TAG_ASSISTED},
        {"num":"7.3", "title":"ISO/IEC 27001", "tag":TAG_ASSISTED},
        {"num":"7.4", "title":"PCI DSS (If Applicable)", "tag":TAG_ASSISTED},
    ]},
    {"num":"Appendices", "title":"Appendices", "tag":None, "children":[
        {"num":"Appendix A", "title":"Detailed Scope and Asset Inventory", "tag":TAG_MANUAL},
        {"num":"Appendix B", "title":"Inherent Limitations", "tag":TAG_ASSISTED},
        {"num":"Appendix C", "title":"Risk Rating Definitions and Methodology", "tag":TAG_ASSISTED},
        {"num":"Appendix D", "title":"Test Methodologies and Tools (overview)", "tag":TAG_AUTOMATABLE},
        {"num":"Appendix E", "title":"Evidence Tables and Screenshots", "tag":TAG_AUTOMATABLE},
        {"num":"Appendix F", "title":"Raw Scan Outputs and Parser Logs", "tag":TAG_AUTOMATABLE},
        {"num":"Appendix G", "title":"Reproduction Steps and Proof-of-Concepts", "tag":TAG_ASSISTED},
        {"num":"Appendix H", "title":"Glossary and Acronyms", "tag":TAG_ASSISTED},
    ]},
]

# =============================== Report Assembly Helpers ===============================
def build_toc_list(toc: List[Dict[str, Any]]) -> List[str]:
    lines = []
    lines.append("Table of Contents")
    for s in toc:
        if s["num"] == "Appendices":
            lines.append(f"- {s['num']} – {s['title']}")
            for c in s.get("children", []):
                lines.append(f"  - {c['num']} – {c['title']} ({c['tag']})")
        else:
            lines.append(f"- {s['num']} {s['title']}")
            for c in s.get("children", []):
                tag = f" ({c['tag']})" if c.get("tag") else ""
                lines.append(f"  - {c['num']} {c['title']}{tag}")
                for cc in c.get("children", []):
                    lines.append(f"    - {cc['num']} {cc['title']} ({cc['tag']})")
    return lines

def brief_section(title: str, tag: str, evidence_ctx: Dict[str, Any]) -> str:
    ctx = f"Section: {title}\nDomain: {evidence_ctx.get('domain')}\nScan start: {evidence_ctx.get('start')}"
    goal = f"Write a concise 1-2 sentence purpose statement for '{title}' in a pentest report. Do not restate findings; point to data sections."
    desc = llm_instruct(ctx, goal) or f"This section summarizes {title} at a high level. See detailed sections for data and evidence."
    if tag == TAG_MANUAL:
        desc += "\nNote: This section is Manual and must be completed by a human analyst."
    return desc

def _surface_risk_score(flags: Dict[str, Any]) -> int:
    # 0-100 score (higher is riskier); simple commercial-friendly metric
    score = 0
    if flags.get('has_outdated'): score += 25
    if flags.get('open_mysql'): score += 20
    if flags.get('open_ftp'): score += 10
    if flags.get('waf') is False: score += 10
    if flags.get('headers_evaluated') and flags.get('missing'):
        score += min(15, 3 * len(flags.get('missing')))
    if flags.get('cookie_issues'): score += min(10, 2 * len(flags.get('cookie_issues')))
    if not flags.get('spf_present'): score += 5
    if not flags.get('dmarc_present'): score += 5
    # Clamp
    return max(0, min(100, score))

def _compliance_readiness_grade(flags: Dict[str, Any]) -> Tuple[str, int]:
    # A simple, commercial-friendly "Compliance Readiness" index derived from inverse risk score
    risk_score = _surface_risk_score(flags)
    readiness = max(0, 100 - risk_score)
    if readiness >= 90: grade = "A"
    elif readiness >= 80: grade = "B"
    elif readiness >= 70: grade = "C"
    elif readiness >= 60: grade = "D"
    else: grade = "F"
    return grade, readiness

def build_at_a_glance_dashboard(evidence_ctx: Dict[str, Any]) -> str:
    domain = evidence_ctx.get("domain","-")
    dnsres = evidence_ctx.get("dns", {})
    ips = ", ".join(dnsres.get("ips", [])[:5]) or "-"
    http = evidence_ctx.get("http", {})
    status = http.get("status","-")
    waf_val = http.get("waf", None)
    waf = "Yes" if waf_val is True else ("No" if waf_val is False else "Unknown")
    frameworks = http.get("frameworks","Unknown")
    tls = evidence_ctx.get("tls", {})
    tls_ver = tls.get("version","-")
    cert = tls.get("cert_not_after","-")
    days = tls.get("cert_days_remaining","-")
    ports = evidence_ctx.get("ports", [])
    open_ports = ", ".join([f"{p.get('port')}/{p.get('proto')}" for p in ports[:12]]) or "-"
    header_analysis = evidence_ctx.get("header_analysis", {})
    if header_analysis.get("evaluated"):
        header_missing = header_analysis.get("missing", [])
        header_missing_str = ", ".join(header_missing) if header_missing else "-"
    else:
        header_missing_str = "Unknown"
    cookie_issues = [i["cookie"] for i in evidence_ctx.get("cookie_analysis", {}).get("issues", [])]
    cookie_issues_str = ", ".join(cookie_issues) if cookie_issues else "-"
    dns_services = evidence_ctx.get("dns_services", [])
    dns_services_str = ", ".join(sorted({d.get("name","-") for d in dns_services})) or "-"
    final_url = http.get("final_url","-") or "-"
    status_chain = http.get("status_chain", []) or []
    status_chain_str = " -> ".join([str(x) for x in status_chain]) if status_chain else str(status)

    # Optional commercial-friendly Surface Risk Score and Compliance Readiness
    flags = _evidence_flags(evidence_ctx)
    risk_score = _surface_risk_score(flags) if ENABLE_RISK_SCORE else None
    grade, readiness = _compliance_readiness_grade(flags)

    lines = []
    lines.append("| Key | Value |")
    lines.append("|---|---|")
    lines.append(f"| Domain | {domain} |")
    lines.append(f"| Final URL | {final_url} |")
    lines.append(f"| Status chain | {status_chain_str} |")
    lines.append(f"| IPs | {ips} |")
    lines.append(f"| HTTP status | {status} |")
    lines.append(f"| Stack/Frameworks | {frameworks} |")
    lines.append(f"| DNS-inferred services | {dns_services_str} |")
    lines.append(f"| WAF detected | {waf} |")
    lines.append(f"| TLS | {tls_ver} |")
    lines.append(f"| Certificate expiry | {cert} ({days} days) |")
    lines.append(f"| Open ports (sample) | {open_ports} |")
    lines.append(f"| Missing security headers | {header_missing_str} |")
    lines.append(f"| Insecure cookies | {cookie_issues_str} |")
    if risk_score is not None:
        lines.append(f"| External Surface Risk Score (0-100) | {risk_score} |")
    lines.append(f"| Compliance Readiness | {grade} ({readiness}/100) |")
    return "\n".join(lines)

# ---------- Evidence flags for compliance/risk ----------
def _evidence_flags(evidence_ctx: Dict[str, Any]) -> Dict[str, Any]:
    headers = (evidence_ctx.get("http", {}) or {}).get("headers", {}) or {}
    present = evidence_ctx.get("header_analysis", {}).get("present", {})
    missing = evidence_ctx.get("header_analysis", {}).get("missing", []) or []
    evaluated_headers = evidence_ctx.get("header_analysis", {}).get("evaluated", False)
    if not evaluated_headers:
        missing = []
    cookie_issues = [i["cookie"] for i in evidence_ctx.get("cookie_analysis", {}).get("issues", [])]
    ports = evidence_ctx.get("ports", []) or []
    waf_val = (evidence_ctx.get("http", {}) or {}).get("waf", None)
    waf = True if waf_val is True else (False if waf_val is False else None)
    tls = evidence_ctx.get("tls", {}) or {}
    tls_ver = tls.get("version", "")
    cert_days = tls.get("cert_days_remaining", None)

    spf_present = bool(evidence_ctx.get("dns", {}).get("spf"))
    dmarc_present = bool(evidence_ctx.get("dns", {}).get("dmarc"))

    open_ftp = any(p.get("port") == 21 and str(p.get("state","")).lower()=="open" for p in ports)
    open_mysql = any(p.get("port") == 3306 and str(p.get("state","")).lower()=="open" for p in ports)

    outdated_entries = evidence_ctx.get("outdated_entries", []) or []
    outdated_summary = ", ".join([f"{x.get('component')} {x.get('version')} ({x.get('support')})" for x in outdated_entries]) if outdated_entries else ""
    has_outdated = bool(outdated_entries)

    return {
        "present": present,
        "missing": missing,
        "cookie_issues": cookie_issues,
        "open_ftp": open_ftp,
        "open_mysql": open_mysql,
        "waf": waf,  # tri-state
        "cert_days": cert_days,
        "tls_ver": tls_ver,
        "spf_present": spf_present,
        "dmarc_present": dmarc_present,
        "outdated_summary": outdated_summary,
        "has_outdated": has_outdated,
        "headers_evaluated": evaluated_headers
    }

# ---------- Section 7 builder (evidence to compliance) ----------
def build_section7_compliance(evidence_ctx: Dict[str, Any]) -> str:
    f = _evidence_flags(evidence_ctx)
    def row(c1, c2, c3, c4):
        return f"| {c1} | {c2} | {c3} | {c4} |\n"

    waf_str = "present" if f['waf'] is True else ("absent" if f['waf'] is False else "unknown")

    owasp = "### 7.1 OWASP ASVS / Top 10\n"
    owasp += "| Category | Evidence | Status | Notes |\n|---|---|---|---|\n"
    owasp += row("A02:2021 Cryptographic Failures",
                 ("TLS " + (f['tls_ver'] or "-") + "; " + ("HSTS present" if f['present'].get("strict-transport-security") else ("HSTS missing" if f['headers_evaluated'] else "HSTS unknown"))),
                 ("Gap" if (f['headers_evaluated'] and not f['present'].get("strict-transport-security")) else ("Strength" if f['present'].get("strict-transport-security") else "Unknown")),
                 "Enable HSTS to enforce HTTPS; verify cipher policy.")
    owasp += row("A04:2021 Insecure Design",
                 ("Missing headers: " + (", ".join(f['missing']) if f['missing'] else ("-" if f['headers_evaluated'] else "unknown"))),
                 ("Gap" if (f['headers_evaluated'] and f['missing']) else ("Strength" if f['headers_evaluated'] and not f['missing'] else "Unknown")),
                 "Add security headers to harden client-side protections.")
    owasp += row("A05:2021 Security Misconfiguration",
                 ("WAF: " + waf_str + "; Server banners observed via HTTP: " + ("yes" if f['headers_evaluated'] else "no")),
                 ("Gap" if f['waf'] is False else ("Strength" if f['waf'] is True else "Unknown")),
                 "Deploy WAF; restrict admin/database ports; sanitize headers.")
    owasp += row("A06:2021 Vulnerable and Outdated Components",
                 (f['outdated_summary'] or "-"),
                 ("Gap" if f['has_outdated'] else "Unknown"),
                 "Upgrade runtimes/frameworks to supported versions; patch regularly.")
    owasp += row("A09:2021 Security Logging and Monitoring Failures",
                 ("No telemetry observable from surface scan"),
                 "Unknown",
                 "Confirm SIEM/logging coverage; alerting on auth and WAF events.")
    owasp += row("Other (A01/A03/A07/A08/A10)",
                 "Authentication and server-side controls not assessed in surface-only scan.",
                 "Unknown",
                 "Perform authenticated testing and code review.")
    owasp += "\n"

    nist = "### 7.2 NIST CSF / NIST 800-53\n"
    nist += "| CSF/Control | Evidence | Status | Recommendation |\n|---|---|---|---|\n"
    nist += row("PR.DS (SC-8) – Data Security (Encryption in Transit)",
                f"TLS {f['tls_ver'] or '-'} in use (handshake status may be limited)",
                ("Strength" if f['tls_ver'] else "Unknown"),
                "Maintain TLS 1.2+; prefer TLS 1.3; renew certificates on time.")
    nist += row("PR.DS (SC-23) – Session/Token Protections",
                ("Cookies missing flags: " + (", ".join(f['cookie_issues']) if f['cookie_issues'] else "-")),
                ("Gap" if f['cookie_issues'] else "Strength"),
                "Set Secure, HttpOnly, SameSite on sensitive cookies.")
    nist += row("PR.PT (SC-7/SI-4) – Protective Technology",
                ("WAF " + waf_str),
                ("Gap" if f['waf'] is False else ("Strength" if f['waf'] is True else "Unknown")),
                "Deploy WAF or equivalent L7 protections.")
    nist += row("ID.AM – Asset Management",
                "Discovered domain/IP and open ports (inventory sample)",
                "Informational",
                "Maintain complete asset inventory and service catalog.")
    nist += row("SI-2 – Flaw Remediation",
                (f"Outdated components: {f['outdated_summary'] or '-'}"),
                ("Gap" if f['has_outdated'] else "Unknown"),
                "Upgrade to supported versions; patch cycle automation.")
    nist += row("DE.CM – Security Continuous Monitoring",
                "Monitoring not observable from surface",
                "Unknown",
                "Confirm centralized logging, retention, and alerting.")
    nist += "\n"

    iso = "### 7.3 ISO/IEC 27001\n"
    iso += "| Annex A Control | Observation | Status | Action |\n|---|---|---|---|\n"
    iso += row("A.10 (Cryptographic Controls)",
               f"TLS {f['tls_ver'] or '-'}; HSTS " + ("present" if f['present'].get("strict-transport-security") else ("missing" if f['headers_evaluated'] else "unknown")),
               ("Gap" if (f['headers_evaluated'] and not f['present'].get("strict-transport-security")) else ("Strength" if f['present'].get("strict-transport-security") else "Unknown")),
               "Enable HSTS and maintain strong TLS configuration.")
    iso += row("A.12.6 (Technical Vulnerability Mgmt)",
               (f"Outdated/EOL: {f['outdated_summary'] or '-'}"),
               ("Gap" if f['has_outdated'] else "Strength"),
               "Track/patch EOL components; upgrade unsupported frameworks.")
    iso += row("A.13.1 (Network Security)",
               ("WAF " + waf_str),
               ("Gap" if f['waf'] is False else ("Strength" if f['waf'] is True else "Unknown")),
               "Restrict management/DB ports; implement WAF.")
    iso += row("A.9 (Access Control)",
               "Auth model not assessed; Internet-exposed services observed",
               "Unknown",
               "Harden service access; enforce least privilege; MFA.")
    iso += row("A.14 (System Acquisition/Dev and Maintenance)",
               ("Missing headers: " + (", ".join(f['missing']) if f['missing'] else ("-" if f['headers_evaluated'] else "unknown"))),
               ("Gap" if (f['headers_evaluated'] and f['missing']) else ("Strength" if f['headers_evaluated'] and not f['missing'] else "Unknown")),
               "Bake security headers and cookie hardening into SDLC.")
    iso += "\n"

    pci = "### 7.4 PCI DSS (If Applicable)\n"
    pci += "| Requirement | Evidence | Status | Notes |\n|---|---|---|---|\n"
    pci += row("Req 4 – Encrypt transmission",
               f"TLS {f['tls_ver'] or '-'}; HSTS " + ("present" if f['present'].get("strict-transport-security") else ("missing" if f['headers_evaluated'] else "unknown")),
               ("Partial" if (f['headers_evaluated'] and not f['present'].get("strict-transport-security")) else ("Strength" if f['present'].get("strict-transport-security") else "Unknown")),
               "Enforce HTTPS with HSTS; verify strong cipher suites.")
    pci += row("Req 6.2 – Patch critical components",
               (f"Outdated/EOL: {f['outdated_summary'] or '-'}"),
               ("Gap" if f['has_outdated'] else "Strength"),
               "Upgrade unsupported runtimes; maintain patch cadence.")
    pci += row("Req 6.6 – WAF or code review for web apps",
               "WAF " + waf_str,
               ("Gap" if f['waf'] is False else ("Strength" if f['waf'] is True else "Unknown")),
               "Deploy WAF or demonstrate equivalent secure code review.")
    pci += row("Req 1.3 – Restrict public access",
               "Exposed services: -",
               ("Strength"),
               "Limit to trusted sources; segment networks.")
    pci += row("Req 10 – Logging and monitoring",
               "Not observable from surface scan",
               "Unknown",
               "Confirm centralized logging, retention, and alerting.")
    pci += "\n"

    return owasp + nist + iso + pci

# ---------- Executive/management helpers ----------
def _overall_risk_rating(flags: Dict[str, Any], findings: List[Dict[str, Any]]) -> str:
    sev_map = {"low": 1, "medium": 2, "high": 3}
    max_score = 0
    for f in findings or []:
        s = sev_map.get(str(f.get("severity","")).lower(), 0)
        if s > max_score:
            max_score = s
    if flags.get('has_outdated'):
        max_score = max(max_score, 3)
    if flags.get('open_mysql'):
        max_score = max(max_score, 3)
    if flags.get('open_ftp'):
        max_score = max(max_score, 2)
    if flags.get('waf') is False:
        max_score = max(max_score, 2)
    if flags.get('headers_evaluated') and 'strict-transport-security' in (flags.get('missing') or []):
        max_score = max(max_score, 2)
    return "High" if max_score >= 3 else ("Medium" if max_score >= 2 else "Low")

def build_section14_exec(evidence_ctx: Dict[str, Any], findings: List[Dict[str, Any]]) -> str:
    f = _evidence_flags(evidence_ctx)
    overall = _overall_risk_rating(f, findings)
    grade, readiness = _compliance_readiness_grade(f)
    bullets = []

    if f['has_outdated']:
        bullets.append(f"Outdated/EOL components: {f['outdated_summary']} – unsupported components raise exploit likelihood.")
    top = [x for x in findings if str(x.get("severity","")).lower() in ("high","medium")][:6]
    for it in top:
        marker = it.get("evidence_marker") or it.get("tech") or "-"
        bullets.append(f"{it.get('title')} – {it.get('desc','')[:100]} (Evidence: {marker})")
    if f['headers_evaluated'] and f['missing']:
        bullets.append("Missing security headers: " + ", ".join(f['missing']))
    if f['cookie_issues']:
        bullets.append("Cookie issues: " + ", ".join(f['cookie_issues']))
    if f['waf'] is False:
        bullets.append("No WAF detected.")
    if not f['spf_present']: bullets.append("SPF record missing.")
    if not f['dmarc_present']: bullets.append("DMARC record missing.")
    if f['open_mysql']:
        bullets.append("Database port 3306/tcp publicly reachable.")
    if f['open_ftp']:
        bullets.append("FTP 21/tcp exposed.")
    if not bullets:
        bullets.append("No material exposures confirmed by automation.")

    recs = []
    if f['has_outdated']:
        recs.append("Upgrade or replace EOL/Unsupported frameworks and runtimes; apply vendor patches.")
    if f['headers_evaluated'] and f['missing']:
        recs.append("Enable HSTS, CSP, X-Frame-Options, X-Content-Type-Options, Referrer-Policy, and Permissions-Policy.")
    if f['cookie_issues']:
        recs.append("Set Secure, HttpOnly, SameSite on all session/CSRF cookies.")
    if f['open_mysql'] or f['open_ftp']:
        recs.append("Restrict admin/DB/legacy protocols to trusted sources or VPN; segment networks.")
    if f['waf'] is False:
        recs.append("Deploy a WAF with baseline OWASP CRS policies.")
    if not f['spf_present'] or not f['dmarc_present']:
        recs.append("Publish SPF and DMARC DNS records; enforce policy progressively.")
    if not recs:
        recs.append("Maintain patch cadence; keep TLS and dependencies up to date; perform periodic header/cookie audits.")

    lines = []
    if ENABLE_BRAND_FRIENDLY:
        lines.append(f"{VENDOR_NAME} {PRODUCT_NAME} Executive Snapshot")
    lines.append(f"Overall Risk: {overall}")
    lines.append(f"Compliance Readiness: {grade} ({readiness}/100)")
    lines.append("")
    lines.append("Key Results")
    for b in bullets:
        lines.append(f"- {b}")
    lines.append("")
    lines.append("Recommended Remediations (Priority)")
    for r in recs:
        lines.append(f"- {r}")
    return "\n".join(lines)

def build_section15_risk_posture(evidence_ctx: Dict[str, Any], findings: List[Dict[str, Any]]) -> str:
    f = _evidence_flags(evidence_ctx)
    overall = _overall_risk_rating(f, findings)
    exposure = []
    if f['has_outdated']: exposure.append(f"EOL components ({f['outdated_summary']})")
    if f['open_mysql']: exposure.append("Public DB (3306/tcp)")
    if f['open_ftp']: exposure.append("FTP (21/tcp)")
    if f['headers_evaluated'] and f['missing']: exposure.append("Missing headers")
    if f['cookie_issues']: exposure.append("Insecure cookie flags")
    if not f['spf_present']: exposure.append("Missing SPF")
    if not f['dmarc_present']: exposure.append("Missing DMARC")
    if f['waf'] is False: exposure.append("No WAF")
    exposure_str = ", ".join(exposure) or "No significant exposures confirmed."
    lines = []
    lines.append(f"Risk Posture: {overall}")
    lines.append(f"Primary Drivers: {exposure_str}")
    lines.append("Business Impact (typical)")
    lines.append("- Account/session compromise, data exposure, spoofed communications, or service disruption if weaknesses are exploited.")
    lines.append("- Regulatory and contractual risk from outdated components and weak client-side protections.")
    return "\n".join(lines)

def build_section16_root_cause(evidence_ctx: Dict[str, Any]) -> str:
    f = _evidence_flags(evidence_ctx)
    causes = []
    if f['has_outdated']: causes.append("Lifecycle management gaps (framework/runtime upgrades not scheduled).")
    if f['headers_evaluated'] and f['missing']: causes.append("Security headers not enforced at reverse proxy/app layer.")
    if f['cookie_issues']: causes.append("Cookie flag baseline not standardized across app.")
    if f['open_mysql'] or f['open_ftp']: causes.append("Perimeter exposure of admin/DB protocols without strict allow-lists.")
    if f['waf'] is False: causes.append("No L7/WAF control to mitigate web threats.")
    if not causes: causes.append("No systemic causes identified from surface evidence.")
    lines = ["Probable Root Causes"]
    lines.extend([f"- {c}" for c in causes])
    return "\n".join(lines)

def build_section17_next_steps(evidence_ctx: Dict[str, Any]) -> str:
    lines = []
    lines.append("Roadmap (30/60/90 days)")
    lines.append("- 30d: Implement HSTS+headers; harden cookies; restrict DB/FTP exposure; set cert expiry monitoring; publish SPF/DMARC.")
    lines.append("- 60d: Upgrade EOL/Unsupported frameworks and runtimes; deploy WAF; enforce network allow-lists.")
    lines.append("- 90d: Integrate checks into CI/CD; enable SIEM monitoring; periodic external attack surface review.")
    return "\n".join(lines)

# ---------- 3.4 CVSS/Severity ----------
def build_section34_cvss(evidence_ctx: Dict[str, Any], findings: List[Dict[str, Any]]) -> str:
    f = _evidence_flags(evidence_ctx)
    rows = []
    if f['headers_evaluated']:
        if "strict-transport-security" in f['missing']:
            rows.append({"issue":"Missing HSTS","evidence":"Header absent","vector":"AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N","score":5.3,"severity":"Medium"})
        if "content-security-policy" in f['missing']:
            rows.append({"issue":"Missing Content-Security-Policy","evidence":"Header absent","vector":"AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:L/A:N","score":5.3,"severity":"Medium"})
        if "x-frame-options" in f['missing']:
            rows.append({"issue":"Missing X-Frame-Options","evidence":"Header absent","vector":"AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:L/A:N","score":4.3,"severity":"Medium"})
        if "x-content-type-options" in f['missing']:
            rows.append({"issue":"Missing X-Content-Type-Options","evidence":"Header absent","vector":"AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:N/A:N","score":3.1,"severity":"Low"})
        if "referrer-policy" in f['missing']:
            rows.append({"issue":"Missing Referrer-Policy","evidence":"Header absent","vector":"AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:N/A:N","score":2.8,"severity":"Low"})
        if "permissions-policy" in f['missing']:
            rows.append({"issue":"Missing Permissions-Policy","evidence":"Header absent","vector":"AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:N/A:N","score":2.8,"severity":"Low"})
    if f['cookie_issues']:
        rows.append({"issue":"Cookie missing HttpOnly/SameSite/Secure","evidence":", ".join(f['cookie_issues']),"vector":"AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N","score":6.1,"severity":"Medium"})
    if f['open_mysql']:
        rows.append({"issue":"Public MySQL (3306/tcp)","evidence":"Port open","vector":"AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H","score":7.5,"severity":"High"})
    if f['open_ftp']:
        rows.append({"issue":"Public FTP (21/tcp)","evidence":"Port open","vector":"AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:L/A:N","score":6.5,"severity":"Medium"})
    if not f['spf_present']:
        rows.append({"issue":"Missing SPF","evidence":"No SPF TXT record found","vector":"AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:L/A:N","score":3.4,"severity":"Low"})
    if not f['dmarc_present']:
        rows.append({"issue":"Missing DMARC","evidence":"No _dmarc TXT record found","vector":"AV:N/AC:L/PR:N/UI:R/S:U/C:L/I:L/A:N","score":3.4,"severity":"Low"})
    for oe in (evidence_ctx.get("outdated_entries") or []):
        rows.append({
            "issue": f"EOL/Unsupported: {oe.get('component')} {oe.get('version')}",
            "evidence": f"{oe.get('support')} | CVEs: {oe.get('cves') or '-'}",
            "vector": "AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H",
            "score": 8.1,
            "severity": "High"
        })
    for vf in findings or []:
        if vf.get("section_type") == "version_info":
            rows.append({
                "issue": "Version Information Disclosure",
                "evidence": vf.get("evidence_marker") or "-",
                "vector": "AV:N/AC:L/PR:N/UI:N/S:U/C:L/I:N/A:N",
                "score": 3.7,
                "severity": "Low"
            })
            break
    if not rows:
        return "No items available for CVSS estimation."
    out = ["| Issue | Evidence | CVSS v3.1 Vector | Score | Severity |",
           "|---|---|---|---|---|"]
    for r in rows:
        out.append(f"| {r['issue']} | {r['evidence']} | {r['vector']} | {r['score']} | {r['severity']} |")
    return "\n".join(out)

# ---------- Section 5.7 (Outdated components) ----------
def build_section57_outdated(outdated_entries: List[Dict[str, str]]) -> str:
    out_rows = []
    for r in outdated_entries:
        out_rows.append({
            "source": r.get("source","-"),
            "component": r.get("component","-"),
            "observed": r.get("observed","-"),
            "support": r.get("support","Unknown"),
            "cves": r.get("cves","-") or "-",
            "risk": "High" if r.get("support") in ["EOL","Unsupported"] else "Medium",
            "notes": r.get("notes","")
        })
    if not out_rows:
        return "No EOL/Unsupported components identified from banners/headers/page."
    table = ["| Source | Component | Observed | Support | Known CVEs | Risk | Notes |",
             "|---|---|---|---|---|---|"]
    for r in out_rows:
        table.append(f"| {r['source']} | {r['component']} | {r['observed']} | {r['support']} | {r['cves']} | {r['risk']} | {r['notes']} |")
    return "\n".join(table)

# ---------- Risk helpers for tables ----------
def _cookie_row_risk(missing_flags: List[str]) -> str:
    miss = set([m.lower() for m in missing_flags])
    if "secure" in miss and "httponly" in miss:
        return "High"
    if "secure" in miss or "httponly" in miss:
        return "Medium"
    if "samesite" in miss:
        return "Low"
    return "-"

def _header_row_risk(header: str, present: bool) -> str:
    if present:
        return "-"
    critical = {"strict-transport-security", "content-security-policy", "x-frame-options"}
    if header in critical:
        return "Medium"
    return "Low"

# ---------- Section 6 helpers ----------
def build_6_1_prioritized(evidence_ctx: Dict[str, Any], findings: List[Dict[str, Any]]) -> str:
    f = _evidence_flags(evidence_ctx)
    lines = []
    lines.append("### 6.1 Prioritized Fix List (30/60/90 Days)")
    lines.append("#### 30 Days (Immediate)")
    bullets_30 = []
    if f['headers_evaluated'] and f['missing']:
        bullets_30.append("Implement security headers: HSTS, Content-Security-Policy, X-Frame-Options, X-Content-Type-Options, Referrer-Policy, Permissions-Policy.")
    if f['cookie_issues']:
        bullets_30.append("Set Secure, HttpOnly, SameSite on all sensitive cookies (e.g., CSRF/session tokens).")
    if not f['spf_present'] or not f['dmarc_present']:
        bullets_30.append("Publish SPF and DMARC records; start with monitoring and progress to enforcement.")
    for b in (bullets_30 or ["No immediate items beyond general hardening were detected."]):
        lines.append(f"- {b}")

    lines.append("#### 60 Days (Short-Term)")
    bullets_60 = []
    if f['has_outdated']:
        bullets_60.append(f"Upgrade/replace EOL/Unsupported components ({f['outdated_summary']}).")
    if f['open_ftp']:
        bullets_60.append("Eliminate FTP (21/tcp) or replace with SFTP/FTPS; restrict to admin networks if required.")
    if f['open_mysql']:
        bullets_60.append("Restrict MySQL (3306/tcp) to trusted networks or VPN; remove public exposure.")
    if f['waf'] is False:
        bullets_60.append("Deploy a WAF or equivalent Layer 7 protection with baseline rules (OWASP CRS).")
    for b in (bullets_60 or ["No short-term items identified."]):
        lines.append(f"- {b}")

    lines.append("#### 90 Days (Medium-Term)")
    bullets_90 = [
        "Integrate header/cookie checks and dependency scanning into CI/CD (DevSecOps gates).",
        "Establish centralized logging and alerting (SIEM) for web/app, DB, and WAF.",
        "Document asset/service inventory and implement regular external attack surface review."
    ]
    for b in bullets_90:
        lines.append(f"- {b}")

    lines.append("\n#### Consolidated Remediation Statements (Deduplicated)")
    seen = set()
    for fx in findings:
        rec = (fx.get("recommendation") or "").strip()
        if rec and rec not in seen:
            lines.append(f"- {rec}")
            seen.add(rec)
    return "\n".join(lines)

def build_6_2_patch_strategy(evidence_ctx: Dict[str, Any]) -> str:
    f = _evidence_flags(evidence_ctx)
    lines = []
    lines.append("- Maintain inventory of runtimes and frameworks; track EOL dates.")
    if f['has_outdated']:
        lines.append(f"- Prioritize upgrades for: {f['outdated_summary']}. Perform regression testing before production.")
    lines.append("- Establish monthly patch windows; emergency patch path for critical CVEs.")
    lines.append("- Automate dependency checks and vendor bulletin monitoring.")
    return "\n".join(lines)

def build_6_3_hardening(evidence_ctx: Dict[str, Any]) -> str:
    f = _evidence_flags(evidence_ctx)
    lines = []
    lines.append("- Enforce security headers (HSTS, CSP, XFO, XCTO, Referrer-Policy, Permissions-Policy).")
    if f['open_mysql'] or f['open_ftp']:
        lines.append("- Restrict admin/DB protocols to management networks; prefer VPN/bastion access.")
    lines.append("- Sanitize server banners via reverse proxy.")
    if f['waf'] is False:
        lines.append("- Deploy WAF with OWASP CRS baseline; tune for app endpoints.")
    lines.append("- Harden cookie attributes (Secure, HttpOnly, SameSite).")
    return "\n".join(lines)

def build_6_4_sdlc(evidence_ctx: Dict[str, Any]) -> str:
    return "\n".join([
        "- Add security header and cookie flag checks to CI/CD.",
        "- Integrate SAST/DAST and dependency scans on PR and release.",
        "- Gate production deploys on critical vulnerability budget.",
        "- Maintain threat models and abuse-case tests for major features."
    ])

def build_6_5_monitoring(evidence_ctx: Dict[str, Any]) -> str:
    f = _evidence_flags(evidence_ctx)
    lines = []
    lines.append("- Centralize logs (WAF/proxy/app/DB); enable anomaly alerts.")
    if isinstance(f['cert_days'], int) and f['cert_days'] is not None:
        lines.append("- Monitor certificate expiry with paging alerts.")
    if f['waf'] is False:
        lines.append("- Add L7 telemetry (WAF) to detect/mitigate attacks.")
    lines.append("- Periodic external surface reviews and port hygiene.")
    return "\n".join(lines)

def build_prioritized_remediation(findings: List[Dict[str, Any]], evidence_ctx: Dict[str, Any]) -> str:
    seen = set()
    out = []
    for f in findings:
        rec = (f.get("recommendation") or "").strip()
        if not rec:
            continue
        if rec in seen:
            continue
        seen.add(rec)
        out.append(f"- {rec}")
    if not out:
        out = ["- Maintain patch cadence; keep TLS and dependencies up to date; perform periodic header/cookie audits."]
    return "\n".join(out)

# ---------- Outdated component collection ----------
def collect_outdated_entries_and_findings(versioned_headers: List[Dict[str, str]], versioned_ports: List[Dict[str, str]], versioned_page: List[Dict[str, str]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    outdated_entries: List[Dict[str, Any]] = []
    findings: List[Dict[str, Any]] = []
    entries = []
    # Merge (page has priority for CMS)
    entries.extend(versioned_page or [])
    entries.extend(versioned_headers or [])
    entries.extend(versioned_ports or [])

    # Deduplicate by (component, version, source, observed)
    seen = set()
    merged = []
    for e in entries:
        key = (e.get("component",""), e.get("version",""), e.get("source",""), e.get("observed",""))
        if key in seen:
            continue
        seen.add(key)
        merged.append(e)

    for e in merged:
        comp = (e.get("component","") or "").strip()
        ver = (e.get("version","") or "").strip()
        if not (comp):
            continue
        # Even if version is unknown, ask LLM (will return Unknown) but keep entry to display presence
        assess = llm_support_vuln_check(comp, ver, context=f"origin: {e.get('source','')}, observed: {e.get('observed','')[:180]}")
        if assess.get("support_status") in ["EOL","Unsupported"]:
            cves = ", ".join([v.get("id","") for v in assess.get("known_vulnerabilities", []) if v.get("id")]) or "-"
            row = {
                "source": e.get("source","-"),
                "component": comp,
                "version": ver,
                "observed": e.get("observed",""),
                "support": assess.get("support_status"),
                "cves": cves,
                "notes": assess.get("notes","")
            }
            outdated_entries.append(row)
            finding = {
                "title": f"Outdated Component: {comp} {ver or ''} ({assess.get('support_status')})".strip(),
                "severity": "High",
                "evidence": f"Observed: {e.get('observed','')} (from {e.get('source','-')}). Support status: {assess.get('support_status')}.",
                "ref": "-",
                "source": e.get("source","-"),
                "confirmed": True,
                "tech": comp,
                "port": "-",
                "evidence_marker": e.get('observed','')[:120],
                "recommendation": get_recommendation({"title": "outdated component"}),
                "llm_explanation": llm_instruct(f"{comp} {ver or '(version unknown)'} is {assess.get('support_status')}", "Explain risks of running EOL/Unsupported components on internet-facing systems in 3 bullets."),
                "desc": f"{comp} {ver or ''} flagged as {assess.get('support_status')}. CVEs: {cves}",
                "risk": "High",
                "section_type": "outdated"
            }
            findings.append(finding)
    return outdated_entries, findings

# =============================== Main Orchestrator ===============================
def agentic_pentest_llm(context: Dict[str, Any]) -> Dict[str, Any]:
    global MASTER_EVIDENCE
    MASTER_EVIDENCE = []

    raw_domain_input = context.get("domain") or context.get("args", {}).get("domain") or ""
    domain = sanitize_domain(raw_domain_input)

    if not domain:
        log_evidence("Input", f"Raw domain input invalid or unsupported: {raw_domain_input}")
        return {"reply": "Invalid domain. Please provide a hostname like 'example.com' or URL like 'https://example.com'.",
                "error": "Invalid domain."}

    if raw_domain_input and raw_domain_input != domain:
        log_evidence("Input", f"Domain normalized: {raw_domain_input} -> {domain}")

    start = datetime.utcnow().isoformat() + "Z"

    # Recon/scanning
    subs = recon_subdomains(domain)
    dnsres = recon_dns(domain)
    asnres = recon_asn(domain)
    dns_services = infer_providers_from_dns(dnsres)
    ip = (dnsres.get("ips", []) or [None])[0]
    ports = port_scan(ip) if ip else []
    http_res = probe_http(domain)
    tls_details = http_res.get("tls", {})
    header_analysis = analyze_security_headers(http_res.get("headers", {}))
    cookie_analysis = analyze_cookie_flags(http_res.get("set_cookies_raw", []))

    # Versioned components (from target only)
    versioned_ports = versioned_entries_from_ports(ports)
    versioned_headers = versioned_entries_from_headers(http_res.get("headers", {}))
    versioned_page = versioned_entries_from_page(http_res)

    # Outdated/EOL aggregation and corresponding findings
    outdated_entries, outdated_findings = collect_outdated_entries_and_findings(versioned_headers, versioned_ports, versioned_page)

    # Findings
    confirmed_findings: List[Dict[str, Any]] = []
    all_vulns: List[Dict[str, Any]] = []

    # CVE enrichment from banners/headers/stack report (limit for brevity)
    tech_candidates = []
    header_keys = ['server','x-powered-by','via','set-cookie','x-aspnet-version','x-aspnetmvc-version','x-generator']
    for k in header_keys:
        v = http_res.get("headers", {}).get(k)
        if v: tech_candidates.append(v.strip())
    # Incorporate rich stack elements (top ones)
    stack_report = http_res.get("stack_report") or {}
    for cat in ["cms", "frameworks", "server", "languages", "cdn"]:
        for it in stack_report.get(cat, [])[:2]:
            tech_candidates.append(it["name"])
    # Include page-detected components
    for it in (http_res.get("detected_versions") or []):
        if it.get("component"):
            if it.get("version"):
                tech_candidates.append(f"{it['component']} {it['version']}")
            else:
                tech_candidates.append(it['component'])

    techs = list(set([t for t in tech_candidates if t]))

    for tech in techs[:10]:  # cap to reduce API/calls
        found_version = ""
        m = re.search(r"([a-zA-Z0-9\.\-_ ]+)[/ ]?(\d+(?:\.\d+)*)", tech)
        if m:
            tech_short, found_version = m.group(1).strip(), m.group(2).strip()
        else:
            tech_short = tech
        cve_results = lookup_vulns(tech_short, found_version) if found_version else lookup_vulns(tech_short)
        for cve in cve_results[:8]:
            llm_expl = llm_instruct(cve.get("summary",""), f"Explain business risk and typical impact for {cve.get('cve',cve.get('id',''))} given external web exposure. Keep to 4-6 bullet points.")
            finding = {
                "title": cve.get('cve') or cve.get("id","CVE"),
                "severity": "High" if "remote" in (cve.get("summary","") or "").lower() else "Medium",
                "evidence": f"Detected technology: {tech_short} {found_version}\n{cve.get('summary','')}",
                "ref": cve.get("cve") or cve.get("id","-"),
                "source": "cve.circl.lu",
                "confirmed": True,
                "tech": tech_short,
                "port": "-",
                "evidence_marker": f"{tech_short} {found_version}".strip() if found_version else tech_short,
                "recommendation": get_recommendation({"title": cve.get('cve','CVE'), "evidence_marker": tech_short}),
                "llm_explanation": llm_expl,
                "desc": cve.get("summary", ""),
                "risk": "High" if "remote" in (cve.get("summary","") or "").lower() else "Medium"
            }
            finding['section_type'] = guess_section_type(finding)
            confirmed_findings.append(finding)

    # Open ports -> potential issues
    for o in ports:
        svc = (o.get("service") or o.get("product") or "(unknown)").strip()
        ver = o.get("version", "").strip()
        marker = (svc+" "+ver).strip().lower() if ver else svc.lower()
        if o.get("state"):
            llm_port = llm_instruct(f"Open port {o.get('port')} running {svc} {ver}.", "Explain why this open port matters and how to remediate in 3-5 bullets.")
            finding = {
                "title": f"Open Port: {o.get('port')} ({svc})",
                "severity": "Medium" if o.get("port") in (21,23,80,110,143,3306) else "Low",
                "evidence": f"Service discovered: {svc} {ver} (port {o.get('port', '')}) : {o.get('state')}",
                "ref": "-",
                "source": "scan",
                "confirmed": False,
                "tech": svc,
                "port": o.get("port"),
                "evidence_marker": marker,
                "recommendation": get_recommendation({"title": "Open Port", "port": o.get('port')}),
                "llm_explanation": llm_port,
                "desc": f"Service discovered: {svc} {ver}.",
                "risk": "Medium" if o.get("port") in (21,23,80,110,143,3306) else "Low"
            }
            finding['section_type'] = guess_section_type(finding)
            all_vulns.append(finding)

    # Automatable analyses (only if headers evaluated)
    cookie_findings = []
    if header_analysis.get("evaluated"):
        for issue in cookie_analysis.get("issues", []):
            desc = f"Cookie '{issue['cookie']}' missing: {', '.join(issue['missing'])}"
            cookie_findings.append({
                "title": "Insecure Cookie Parameter",
                "severity": "Medium",
                "evidence": issue.get("raw",""),
                "ref": "-",
                "source": "http_headers",
                "confirmed": True,
                "tech": "HTTP Cookie",
                "port": 443,
                "evidence_marker": issue['cookie'],
                "recommendation": get_recommendation({"title": "cookie", "evidence_marker": issue['cookie']}),
                "llm_explanation": llm_instruct(desc, "Explain why missing Secure/HttpOnly/SameSite is risky and how to fix."),
                "desc": desc,
                "risk": "Medium",
                "section_type": "cookie"
            })
    confirmed_findings.extend(cookie_findings)

    header_findings = []
    if header_analysis.get("evaluated"):
        for missing in header_analysis.get("missing", []):
            hdr_title = f"Missing Security Header: {missing}"
            header_findings.append({
                "title": hdr_title,
                "severity": "Low" if missing in ["x-content-type-options","referrer-policy","permissions-policy"] else "Medium",
                "evidence": f"Header '{missing}' not observed in HTTP responses.",
                "ref": "-",
                "source": "http_headers",
                "confirmed": True,
                "tech": "HTTP",
                "port": 443,
                "evidence_marker": missing,
                "recommendation": get_recommendation({"title": "header", "evidence_marker": missing}),
                "llm_explanation": llm_instruct(hdr_title, f"Explain the purpose of {missing} and secure recommended value in 3-4 bullets."),
                "desc": hdr_title,
                "risk": "Medium" if missing in ["strict-transport-security","content-security-policy","x-frame-options"] else "Low",
                "section_type": "header"
            })
    confirmed_findings.extend(header_findings)

    tls_findings = []
    if tls_details.get("status") == "ok":
        tls_ver = tls_details.get("version","")
        if tls_ver and str(tls_ver).lower().startswith("tlsv1.") and tls_ver in ["TLSv1", "TLSv1.1"]:
            tls_findings.append({
                "title": f"Weak TLS Protocol Negotiated: {tls_ver}",
                "severity": "High",
                "evidence": f"Observed TLS version: {tls_ver}",
                "ref": "-",
                "source": "tls_probe",
                "confirmed": True,
                "tech": "TLS",
                "port": 443,
                "evidence_marker": tls_ver,
                "recommendation": get_recommendation({"title": "tls", "evidence_marker": tls_ver}),
                "llm_explanation": llm_instruct(f"Observed TLS version {tls_ver}", "Explain risks of legacy TLS and how to enforce TLS 1.2/1.3."),
                "desc": f"Legacy TLS {tls_ver} observed.",
                "risk": "High",
                "section_type": "ssl"
            })
        try:
            days_left = int(tls_details.get("cert_days_remaining", 9999) or 9999)
            if days_left < 30:
                tls_findings.append({
                    "title": "TLS Certificate Nearing Expiry",
                    "severity": "Medium",
                    "evidence": f"Certificate expires on {tls_details.get('cert_not_after')} ({days_left} days remaining).",
                    "ref": "-",
                    "source": "tls_probe",
                    "confirmed": True,
                    "tech": "TLS",
                    "port": 443,
                    "evidence_marker": "cert_expiry",
                    "recommendation": "Renew the TLS certificate prior to expiry; implement automated issuance/renewal (e.g., ACME).",
                    "llm_explanation": llm_instruct("Certificate nearing expiry", "Explain risks of expiring TLS certs and renewal best practices in 3 bullets."),
                    "desc": "TLS certificate approaching expiry.",
                    "risk": "Medium",
                    "section_type": "ssl"
                })
        except Exception:
            pass
    confirmed_findings.extend(tls_findings)

    ver_info_findings = []
    server_header = http_res.get("headers", {}).get("server", "")
    if header_analysis.get("evaluated") and server_header and re.search(r'\d+\.\d+', server_header):
        ver_info_findings.append({
            "title": "Version Information Disclosure",
            "severity": "Low",
            "evidence": f"Server header discloses version: {server_header}",
            "ref": "-",
            "source": "http_headers",
            "confirmed": True,
            "tech": "HTTP Server",
            "port": 443,
            "evidence_marker": server_header,
            "recommendation": "Remove or obfuscate version banners; use reverse proxy to sanitize headers.",
            "llm_explanation": llm_instruct(server_header, "Explain the risks of version disclosure and hardening approaches."),
            "desc": "Server header reveals product/version.",
            "risk": "Low",
            "section_type": "version_info"
        })
    confirmed_findings.extend(ver_info_findings)

    dns_findings = []
    if not (dnsres.get("spf") or []):
        dns_findings.append({
            "title": "Missing SPF Record",
            "severity": "Medium",
            "evidence": "SPF record not found in DNS TXT.",
            "ref": "-",
            "source": "dns",
            "confirmed": True,
            "tech": "DNS",
            "port": "-",
            "evidence_marker": "SPF",
            "recommendation": "Publish an SPF record that authorizes legitimate mail sources and uses -all or ~all as appropriate.",
            "llm_explanation": llm_instruct("SPF missing", "Explain risks of missing SPF and recommended record patterns."),
            "desc": "SPF not detected.",
            "risk": "Medium",
            "section_type": "dns"
        })
    if not (dnsres.get("dmarc") or []):
        dns_findings.append({
            "title": "Missing DMARC Record",
            "severity": "Medium",
            "evidence": "DMARC record not found for _dmarc."+domain,
            "ref": "-",
            "source": "dns",
            "confirmed": True,
            "tech": "DNS",
            "port": "-",
            "evidence_marker": "DMARC",
            "recommendation": "Publish a DMARC record with at least p=none for monitoring; progress to quarantine/reject.",
            "llm_explanation": llm_instruct("DMARC missing", "Explain purpose of DMARC and staged policy rollout."),
            "desc": "DMARC not detected.",
            "risk": "Medium",
            "section_type": "dns"
        })
    confirmed_findings.extend(dns_findings)

    # Include outdated findings
    confirmed_findings.extend(outdated_findings)

    # Evidence context for building sections
    evidence_ctx: Dict[str, Any] = {
        "domain": domain,
        "start": start,
        "subs": subs,
        "dns": dnsres,
        "asn": asnres,
        "ports": ports,
        "http": http_res,
        "tls": tls_details,
        "header_analysis": header_analysis,
        "cookie_analysis": cookie_analysis,
        "web_tech": techs,
        "outdated_entries": outdated_entries,
        "dns_services": dns_services
    }

    # =============================== Build Report ===============================
    md: List[str] = []
    md.append(f"# Automated Penetration Test Report for {domain}")
    md.append(f"_Assessment started: {start}_\n")

    if ENABLE_BRAND_FRIENDLY:
        md.append(f"Powered by {VENDOR_NAME} {PRODUCT_NAME}\n")

    md.append("Legend")
    md.append("- Automatable: Code can perform end-to-end reliably.")
    md.append("- Assisted: Code/LLM can generate/detect/draft, but needs human validation or inputs.")
    md.append("- Manual: Primarily human-driven; code can’t reliably do this without significant human action/approval.\n")

    md.extend(build_toc_list(TOC))
    md.append("")

    for section in TOC:
        if section["num"] not in ["1", "2", "3"]:
            continue
        md.append(f"\n## {section['num']} {section['title']}")
        for child in section.get("children", []):
            md.append(f"\n### {child['num']} {child['title']} ({child['tag']})")
            if child['num'] == "1.4":
                md.append(build_section14_exec(evidence_ctx, confirmed_findings + all_vulns))
            elif child['num'] == "1.5":
                md.append(build_section15_risk_posture(evidence_ctx, confirmed_findings + all_vulns))
            elif child['num'] == "1.6":
                md.append(build_section16_root_cause(evidence_ctx))
            elif child['num'] == "1.7":
                md.append(build_section17_next_steps(evidence_ctx))
            elif child['num'] == "3.4":
                md.append(build_section34_cvss(evidence_ctx, confirmed_findings + all_vulns))
            elif child['num'] == "3.6":
                f = _evidence_flags(evidence_ctx)
                lines = []
                lines.append("| Topic | Observation |")
                lines.append("|---|---|")
                lines.append(f"| Transport encryption | TLS {f['tls_ver'] or '-'} active |")
                lines.append(f"| HTTPS enforcement | HSTS {'present' if f['present'].get('strict-transport-security') else ('missing' if f['headers_evaluated'] else 'unknown')} |")
                lines.append(f"| Cookies | Issues: {', '.join(f['cookie_issues']) if f['cookie_issues'] else 'none observed'} |")
                lines.append(f"| Exposure | Services open: {', '.join([str(p.get('port')) for p in evidence_ctx.get('ports',[])]) or '-'} |")
                md.append("\n".join(lines))
            elif child['num'] == "3.7":
                md.append("- Surface-only, unauthenticated testing\n- No credentialed or code-level review\n- Protocol/banner-based inferences only\n- Dynamic app behavior (auth flows, role controls) not exercised")
            else:
                md.append(brief_section(f"{child['num']} {child['title']}", child['tag'], evidence_ctx))
            for gchild in child.get("children", []):
                md.append(f"\n#### {gchild['num']} {gchild['title']} ({gchild['tag']})")
                md.append(brief_section(f"{gchild['num']} {gchild['title']}", gchild['tag'], evidence_ctx))

    md.append(f"\n## 4 Technical Summary")
    md.append("\n### 4.0 Narrative Technical Summary")
    tech_summary = llm_instruct("\n".join([f.get("evidence","") for f in confirmed_findings]) or "No confirmed CVEs.", "Technical summary for engineering. Focus on actionable fixes and exposure.")
    md.append(tech_summary or "This section summarizes technical findings and recommended actions based on surface evidence.")

    md.append("\n### 4.1 Target Overview and Attack Surface (Dashboard)")
    md.append(build_at_a_glance_dashboard(evidence_ctx))

    md.append("\n### 4.2 Technology Stack and Versions (with LLM Support)")
    # DNS-inferred third-party services
    svc_table = ["#### 4.2.1 DNS-Inferred Third-Party Services",
                 "| Service | Source | Pattern/Evidence |",
                 "|---|---|---|"]
    if evidence_ctx.get("dns_services"):
        for s in evidence_ctx["dns_services"]:
            svc_table.append(f"| {s['name']} | {s['source']} | {s['pattern']} |")
    else:
        svc_table.append("| - | - | - |")
    md.append("\n".join(svc_table))

    # Rich stack detail table for commercial clarity
    stack_detail = http_res.get("stack_report") or {}
    st = ["\n#### 4.2.2 Application Stack Fingerprint (Heuristic)"]
    st.append("| Category | Detected | Confidence | Evidence |")
    st.append("|---|---|---|---|")
    for cat in ["cms","frameworks","languages","server","cdn"]:
        for it in stack_detail.get(cat, []):
            st.append(f"| {cat.upper()} | {it.get('name','-')} | {int(it.get('confidence',0)*100)}% | {it.get('evidence','-')} |")
    if len(st) == 3:
        st.append("| - | - | - | - |")
    md.append("\n".join(st))

    # Page-first CMS/Framework versions
    md.append("\n#### 4.2.3 Detected CMS/Framework Versions (Page-first)")
    pv = http_res.get("detected_versions") or []
    if pv:
        vtable = ["| Component | Version | Source | Evidence |", "|---|---|---|---|"]
        # Deduplicate by component/version
        seen = set()
        for it in pv:
            key = (it.get("component",""), it.get("version",""), it.get("source",""), it.get("observed",""))
            if key in seen:
                continue
            seen.add(key)
            vtable.append(f"| {it.get('component','-')} | {it.get('version') or '-'} | {it.get('source','page')} | {it.get('observed','')[:90]} |")
        md.append("\n".join(vtable))
    else:
        md.append("No page-level CMS/framework versions detected.")

    md.append("\n### 4.3 Network Summary (Hosts, Open Ports, Services, Products)")
    md.append("\n#### 4.3.1 Open Ports and Services")
    port_table = "| Port | Proto | Service | Product | Version | State |\n|------|-------|---------|---------|---------|-------|\n"
    for p in ports:
        port_table += f"| {p.get('port','')} | {p.get('proto','')} | {p.get('service','')} | {p.get('product','')} | {p.get('version','')} | {p.get('state','')} |\n"
    md.append(port_table)

    def build_llm_table(entries: List[Dict[str, str]], title: str) -> str:
        if not entries:
            return f"### {title}\nNo versioned components detected.\n"
        table = f"### {title}\n| Source | Component | Observed | Support (LLM) | Known Vulns | Notes |\n|---|---|---|---|---|---|\n"
        # Deduplicate components+versions to reduce LLM calls
        uniq = []
        seen = set()
        for e in entries:
            key = (e.get("component",""), e.get("version",""), e.get("source",""), e.get("observed",""))
            if key in seen:
                continue
            seen.add(key)
            uniq.append(e)
        for e in uniq[:30]:
            assess = llm_support_vuln_check(e["component"], e.get("version",""), context=f"origin: {e.get('source','')}, observed: {e.get('observed','')}")
            cves = ", ".join([v.get("id","") for v in assess.get("known_vulnerabilities", []) if v.get("id")])
            table += f"| {e.get('source','-')} | {e.get('component','-')} | {e.get('observed','-')[:90]} | {assess.get('support_status','Unknown')} | {cves or '-'} | {assess.get('notes','')} |\n"
        return table + "\n"

    md.append(build_llm_table(versioned_ports, "Versioned Network Services – LLM Assessment"))
    md.append(build_llm_table(versioned_headers, "Versioned Components from HTTP Headers – LLM Assessment"))
    md.append(build_llm_table(versioned_page, "Versioned Components from Landing Page – LLM Assessment"))

    md.append("\n### 4.4 Application Summary (Headers, TLS, WAF)")
    md.append("\n#### 4.4.1 HTTP Header Summary")
    app_table = "| Header | Value |\n|--------|-------|\n"
    if http_res.get("headers"):
        for k,v in (http_res.get("headers", {}) or {}).items():
            app_table += f"| {k} | {str(v)[:120]} |\n"
    else:
        app_table += "| (no HTTP response headers captured) | - |\n"
    md.append(app_table)

    md.append("\n#### 4.4.2 TLS Summary")
    tls_table = "| Field | Value |\n|---|---|\n"
    for k in ["status","version","cipher","cert_not_after","cert_days_remaining","issuer","subject"]:
        tls_table += f"| {k} | {tls_details.get(k,'-')} |\n"
    md.append(tls_table)

    md.append("\n### 4.5 DNS and Email Security Posture")
    dns_table = "| Key | Values |\n|-----|--------|\n"
    for k in ['ips','ptr','mx','spf','dmarc']:
        val = ", ".join(dnsres.get(k, [])) if k in dnsres and dnsres.get(k) else "-"
        dns_table += f"| {k.upper()} | {val} |\n"
    md.append(dns_table)

    high_findings = [f for f in confirmed_findings if str(f.get("severity","")).lower() == "high"][:10]
    md.append("\n### 4.6 High-Risk Findings Summary and Quick Wins")
    if high_findings:
        for fnd in high_findings:
            md.append(f"- {fnd['title']} – {fnd.get('desc','')[:120]} (Evidence: {fnd.get('evidence_marker','')})")
    else:
        md.append("- No high-risk items confirmed by automation.")

    md.append("\n## 5 Detailed Findings – Penetration Test")
    md.append("\n### 5.0 Findings Summary Table")
    vulntable = "| Finding | Port | Tech | Severity | Evidence | Recommendation |\n|---------|------|------|----------|----------|----------------|\n"
    for fnd in confirmed_findings + all_vulns:
        ev = (fnd.get('evidence') or "")[:140].replace("\n"," ")
        vulntable += f"| {fnd.get('title','-')} | {fnd.get('port','-')} | {fnd.get('tech','-')} | {fnd.get('severity','-')} | {ev} | {fnd.get('recommendation','-')} |\n"
    md.append(vulntable)

    md.append("\n### 5.6 Insecure Cookie Parameters (Automatable)")
    if cookie_analysis.get("parsed") and header_analysis.get("evaluated"):
        ct = "| Cookie | Secure | HttpOnly | SameSite | Missing | Risk |\n|---|---|---|---|---|---|\n"
        for c in cookie_analysis["parsed"]:
            missing = []
            if not c["flags"].get("Secure"): missing.append("Secure")
            if not c["flags"].get("HttpOnly"): missing.append("HttpOnly")
            if not c["flags"].get("SameSite"): missing.append("SameSite")
            risk = _cookie_row_risk(missing)
            ct += f"| {c['name']} | {c['flags'].get('Secure')} | {c['flags'].get('HttpOnly')} | {c['flags'].get('SameSite') or '-'} | {', '.join(missing) or '-'} | {risk} |\n"
        md.append(ct)
    else:
        md.append("No Set-Cookie headers observed or HTTP headers not captured.")

    md.append("\n### 5.7 Outdated and Vulnerable Software in Use (Assisted)")
    md.append(build_section57_outdated(outdated_entries))

    md.append("\n### 5.8 Insecure HTTP Response Header Configuration (Automatable)")
    ht = "| Header | Present | Risk |\n|---|---|---|\n"
    if header_analysis.get("evaluated"):
        for hk, present in header_analysis.get("present", {}).items():
            ht += f"| {hk} | {present} | {_header_row_risk(hk, present)} |\n"
    else:
        ht += "| (not evaluated) | - | - |\n"
    md.append(ht)

    tls_related = [f for f in confirmed_findings if f.get("section_type") == "ssl"]
    if tls_related:
        md.append("\n### 5.9 Insecure SSL/TLS Configuration (Automatable)")
        for fnd in tls_related:
            md.append(f"- {fnd['title']}: {fnd['evidence']}")

    vi_related = [f for f in confirmed_findings if f.get("section_type") == "version_info"]
    if vi_related:
        md.append("\n### 5.14 Version Information Disclosure (Automatable)")
        for fnd in vi_related:
            md.append(f"- {fnd['evidence']}")

    dns_related = [f for f in confirmed_findings if f.get("section_type") == "dns"]
    if dns_related:
        md.append("\n### 5.15 Missing Security DNS Records (Automatable)")
        for fnd in dns_related:
            md.append(f"- {fnd['title']}: {fnd['evidence']}")

    md.append("\n### 5.X Other Categories (Manual/Assisted stubs)")
    md.append("- The following categories require manual validation and are not expanded without direct evidence: XSS, file upload, access control, injection, SSRF, CSRF, session management, password/MFA.")

    md.append("\n## 6 Remediation Plan and Roadmap")
    md.append(build_6_1_prioritized(evidence_ctx, confirmed_findings + all_vulns))
    md.append("\n### 6.2 Dependency and Patch Management Strategy (Assisted)")
    md.append(build_6_2_patch_strategy(evidence_ctx))
    md.append("\n### 6.3 Hardening Standards and Secure Configuration Baselines (Assisted)")
    md.append(build_6_3_hardening(evidence_ctx))
    md.append("\n### 6.4 SDLC and DevSecOps Integration (Assisted)")
    md.append(build_6_4_sdlc(evidence_ctx))
    md.append("\n### 6.5 Monitoring, Detection, and Preventive Controls (Assisted)")
    md.append(build_6_5_monitoring(evidence_ctx))

    md.append("\n## 7 Compliance and Framework Mapping")
    md.append(build_section7_compliance(evidence_ctx))

    md.append("\n## 8 LLM Transparency and Governance Annex")
    md.append("\n### 8.1 Models and Versions Used (Automatable)")
    md.append("- Chat Model: gpt-4.1\n- Tooling used: HTTP client, DNS resolver, TLS probe, optional nmap\n")

    md.append("\n### 8.2 Prompt Templates and System Messages (Automatable)")
    md.append("System Prompt (Sections):")
    md.append(f"```\n{PROMPT_LLMI_INSTRUCT_SYSTEM}\n```")
    md.append("User Prompt Template (Sections):")
    md.append(f"```\n{PROMPT_LLMI_INSTRUCT_USER_TEMPLATE}\n```")
    md.append("System Prompt (Support/Vuln):")
    md.append(f"```\n{PROMPT_SUPPORT_VULN_SYSTEM}\n```")
    md.append("User Prompt Template (Support/Vuln):")
    md.append(f"```\n{PROMPT_SUPPORT_VULN_USER_TEMPLATE}\n```")
    md.append("System Prompt (Tech Stack Guess, constrained):")
    md.append(f"```\n{PROMPT_TECH_SYSTEM}\n```")
    md.append("User Prompt Template (Tech Stack Guess, constrained):")
    md.append(f"```\n{PROMPT_TECH_USER_TEMPLATE}\n```")

    md.append("\n### 8.3 Data Minimization and Redaction Practices (Assisted)")
    md.append("- No PII discovered in surface scan. Ensure minimal token/session data in cookies; avoid sensitive info in logs. Absence of HSTS increases downgrade risk.")

    md.append("\n### 8.4 Output Validation Criteria and Reviewer Sign-Offs (Manual)")
    md.append("- Findings require reviewer validation: evidence present, repro steps clear, impact justified, remediation actionable, and risk mapped to frameworks.")

    md.append("\n### 8.5 Change Log (Model/Prompt Updates and Rationale) (Assisted)")
    md.append("- This run used gpt-4.1 prompts shown above; no mid-run changes applied.")

    md.append("\n### 8.6 Known LLM Limitations and Acceptable Use (Assisted)")
    md.append("- Surface evidence only; no authenticated or intrusive testing. Treat narrative as guidance; verify before production changes.")

    md.append("\n# Appendices")

    md.append("## Appendix A – Detailed Scope and Asset Inventory")
    a_table = "| Item | Values |\n|---|---|\n"
    a_table += f"| Domain | {evidence_ctx.get('domain','-')} |\n"
    a_table += f"| IPs | {', '.join(evidence_ctx.get('dns',{}).get('ips',[]) or ['-'])} |\n"
    a_table += f"| Subdomains (sample) | {', '.join(evidence_ctx.get('subs',[])[:20]) or '-'} |\n"
    a_table += f"| Open Ports | {', '.join([str(p.get('port')) for p in evidence_ctx.get('ports',[])]) or '-'} |\n"
    md.append(a_table)

    md.append("\n## Appendix B – Inherent Limitations")
    md.append("- Unauthenticated, surface-only assessment; internal logic and authenticated flows not exercised.\n- Banner/header inference may differ from actual configs.\n- Open ports observed do not guarantee global reachability; network ACLs unknown.")

    md.append("\n## Appendix C – Risk Rating Definitions and Methodology")
    md.append("- Risk considers likelihood (exposure, ease) and impact (confidentiality/integrity/availability). CVSS v3.1 estimates provided where applicable.")

    md.append("\n## Appendix D – Test Methodologies and Tools (overview)")
    md.append("- Tools used: DNS lookups, HTTP client, TLS probe, optional nmap. This section intentionally omits the agent's runtime/OS/library versions to avoid conflating tooling environment with the target environment.")

    md.append("\n## Appendix E – Evidence Tables")
    deduped = dedupe_evidence(MASTER_EVIDENCE)
    if deduped:
        md.append("| Section | Evidence |\n|---|---|\n")
        for ev in deduped:
            lines = str(ev['detail']).replace("\n", " ").replace("\r"," ")
            md.append(f"| {ev['section']} | {lines[:240]} |")
    else:
        md.append("No evidence captured.")

    md.append("\n## Appendix F – Raw Scan Outputs and Parser Logs")
    raw_sections = {
        "dns": dnsres,
        "asn": asnres,
        "ports": ports[:50],
        "http_headers": {k: (v[:200] if isinstance(v, str) else str(v)) for k,v in (http_res.get('headers', {}) or {}).items()},
        "tls": tls_details,
        "subdomains_sample": subs[:50],
        "dns_inferred_services": dns_services,
        "http_meta": {
            "final_url": http_res.get("final_url"),
            "status_chain": http_res.get("status_chain")
        },
        "stack_report": http_res.get("stack_report"),
        "detected_versions": http_res.get("detected_versions")
    }
    md.append("```json\n" + json.dumps(raw_sections, indent=2)[:12000] + "\n```")

    repro_ctx = "\n".join([f"{f['title']}: {f.get('evidence','')[:200]}" for f in (confirmed_findings[:8])])
    md.append("\n## Appendix G – Reproduction Steps and Proof-of-Concepts")
    md.append(llm_instruct(repro_ctx or "No confirmed vulnerabilities.", "Provide safe, high-level repro steps for the above issues. Avoid destructive actions.") or "- High-level reproduction steps require manual validation.")

    md.append("\n## Appendix H – Glossary and Acronyms")
    md.append(llm_instruct("List common terms: CSP, HSTS, CSRF, IDOR, WAF, TLS, CVE, CVSS, SPF, DKIM, DMARC.", "Write concise definitions as a bullet list.") or "- CSP: Content Security Policy\n- HSTS: HTTP Strict Transport Security")

    raw_report = {
        "domain": domain,
        "executed": start,
        "recon": { "subdomains": subs, "dns": dnsres, "asn": asnres },
        "ports": ports,
        "web_tech": techs,
        "findings_summary": confirmed_findings + all_vulns,
        "frameworks": http_res.get("frameworks"),
        "evidence": deduped,
        "tls": tls_details,
        "outdated_entries": outdated_entries,
        "dns_inferred_services": dns_services,
        "http_meta": {
            "final_url": http_res.get("final_url"),
            "status_chain": http_res.get("status_chain"),
            "stack_report": http_res.get("stack_report"),
            "detected_versions": http_res.get("detected_versions")
        }
    }

    return {
        "reply": "\n".join(md),
        "summary": f"{PRODUCT_NAME} Pentest complete for {domain}",
        "findings": confirmed_findings + all_vulns,
        "raw_report": raw_report
    }

# =============================== CLI Entry ===============================
if __name__ == "__main__":
    import sys
    dom = sys.argv[1] if len(sys.argv) > 1 else input("Domain: ").strip()
    res = agentic_pentest_llm({"domain": dom, "user": "cliuser"})
    print("\n\n=== PenTest Markdown Output ===\n")
    print(res["reply"][:120000])
