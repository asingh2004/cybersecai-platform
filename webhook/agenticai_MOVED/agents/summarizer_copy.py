# File: agents/summarizer.py

import os, csv, re, codecs
from collections import Counter
from datetime import datetime, timedelta, timezone
from agents.cybersec import allrisk_path, highrisk_path, count_files  # using count_files only

EMAIL_RE = re.compile(r'[A-Za-z0-9\._%+-]+@[A-Za-z0-9\.-]+\.[A-Za-z]{2,}')

def _parse_iso(dt_str: str):
    if not dt_str:
        return None
    try:
        # Normalize 'Z' to '+00:00' then parse
        s = dt_str.strip().replace("Z", "+00:00")
        dt = datetime.fromisoformat(s)
        # If parsed datetime is naive, assume UTC
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None

def _load_csv_rows(path):
    if not os.path.isfile(path):
        return []
    with codecs.open(path, 'r', encoding='utf-8-sig') as f:
        reader = csv.DictReader(f)
        return [r for r in reader if any((v or '').strip() for v in r.values())]

def _markdown_table(headers, rows):
    out = [
        "| " + " | ".join(headers) + " |",
        "| " + " | ".join(["-"*max(3, len(h)) for h in headers]) + " |"
    ]
    for r in rows:
        out.append("| " + " | ".join(r) + " |")
    return "\n".join(out)

def agent_summarizer_stats(context):
    """
    Summarizer: High-risk statistics dashboard
    - Clear intro: generated by CyberSecAI and scoped to overall_risk_rating == "High"
    - KPIs and summaries (Total files; High-risk files; Recent High-risk)
    - Breakdowns (classification, subject area, data source) ‚Äî High-risk only
    - Unique permission emails ‚Äî High-risk only
    - Summary by auditor proposed action ‚Äî High-risk only
    - High-risk by jurisdiction (if present)
    - Unique list of up to 50 High-risk files with details (sorted by last_modified desc)
    - Optional files by chosen source (still scoped to High-risk)
    - CSV download links
    Args in context.get('args'):
      - days (int): recent window, default 7
      - data_source (str | None): optional filter for ‚ÄúFiles in Source‚Äù section
    """
    user_id = context.get("user_id")
    args = context.get("args") or {}
    days = int(args.get("days", 7))
    filter_source = (args.get("data_source") or "").strip() or None

    now_utc = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%SZ")

    # Load All and HighRisk CSV rows
    all_rows = _load_csv_rows(allrisk_path(user_id))
    highrisk_rows_raw = _load_csv_rows(highrisk_path(user_id))

    # Filter strictly to overall_risk_rating == "High"
    def is_high(r):
        return (r.get("overall_risk_rating") or "").strip().lower() == "high"
    high_rows = [r for r in highrisk_rows_raw if is_high(r)]
    # If HighRisk CSV is absent or mixed, fall back to filtering all_rows
    if not high_rows and all_rows:
        high_rows = [r for r in all_rows if is_high(r)]

    # Top-line counts
    total_files = count_files(user_id=user_id)
    total_high = len(high_rows)

    # Recent High-risk (last N days) ‚Äî use timezone-aware cutoff
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    recent_high_count = 0
    for r in high_rows:
        lm = _parse_iso(r.get("last_modified") or "")
        if lm and lm >= cutoff:
            recent_high_count += 1

    # Breakdown by classification (High only)
    cls_counter = Counter((r.get("data_classification") or "Unknown").strip() for r in high_rows)
    cls_rows = [[k or "Unknown", str(v)] for k, v in sorted(cls_counter.items(), key=lambda x: (-x[1], x[0] or ""))]

    # Breakdown by subject area (High only)
    subj_counter = Counter((r.get("likely_data_subject_area") or "Unknown").strip() for r in high_rows)
    subj_rows = [[k or "Unknown", str(v)] for k, v in sorted(subj_counter.items(), key=lambda x: (-x[1], x[0] or ""))]

    # Breakdown by data source (High only)
    src_counter = Counter((r.get("data_source") or "Unknown").strip() for r in high_rows)
    src_rows = [[k or "Unknown", str(v)] for k, v in sorted(src_counter.items(), key=lambda x: (-x[1], x[0] or ""))]

    # Optional: Files by chosen source (High only)
    files_by_source_rows = []
    if filter_source:
        files = []
        for r in high_rows:
            if (r.get("data_source") or "").strip().lower() == filter_source.lower():
                files.append([
                    r.get("file_name") or "[unknown]",
                    r.get("data_source") or "",
                    r.get("data_classification") or "",
                    (r.get("overall_risk_rating") or ""),
                    (r.get("last_modified") or "")
                ])
        files_by_source_rows = files[:100]  # separate from the 50-profile list

    # Unique permission emails (High only)
    unique_emails = set()
    for r in high_rows:
        perms = r.get("permissions") or ""
        if "@" in perms:
            for e in EMAIL_RE.findall(perms):
                unique_emails.add(e.lower())
    unique_emails_list = sorted(unique_emails)

    # Summary by auditor proposed action (High only)
    action_counter = Counter((r.get("auditor_proposed_action") or "None").strip() for r in high_rows)
    action_rows = [[k or "None", str(v)] for k, v in sorted(action_counter.items(), key=lambda x: (-x[1], x[0] or ""))]

    # High risk by jurisdiction (if present)
    juris_rows = []
    if any("jurisdiction" in r for r in high_rows):
        juris_counter = Counter((r.get("jurisdiction") or "Unknown").strip() for r in high_rows)
        juris_rows = [[k or "Unknown", str(v)] for k, v in sorted(juris_counter.items(), key=lambda x: (-x[1], x[0] or ""))]

    # Unique list of up to 50 High-risk files (by file_name), sorted by last_modified desc
    seen = set()
    aware_min = datetime.min.replace(tzinfo=timezone.utc)
    high_sorted = sorted(
        high_rows,
        key=lambda r: _parse_iso(r.get("last_modified") or "") or aware_min,
        reverse=True
    )
    high_unique_50 = []
    for r in high_sorted:
        name = (r.get("file_name") or "").strip() or "[unknown]"
        if name.lower() in seen:
            continue
        seen.add(name.lower())
        high_unique_50.append(r)
        if len(high_unique_50) >= 50:
            break

    high_detail_rows = []
    for r in high_unique_50:
        high_detail_rows.append([
            r.get("file_name") or "[unknown]",
            r.get("data_source") or "",
            r.get("data_classification") or "",
            r.get("overall_risk_rating") or "",
            r.get("likely_data_subject_area") or "",
            r.get("last_modified") or ""
        ])

    # Build Markdown dashboard
    sections = []

    # Intro banner
    intro = (
        "## üîé CyberSecAI High‚ÄëRisk Statistics Dashboard\n\n"
        "This dashboard is generated by the Expert Cyber AI Agent (CyberSecAI) after analyzing your organization's files "
        "monitored by the cybersecai.io platform. It presents statistics scoped to files with an overall risk rating of "
        "**‚ÄúHigh‚Äù** (unless otherwise noted), alongside a unique list of upto 50 High‚Äërisk files with details.\n\n"
        f"- üïí Generated: {now_utc} UTC\n"
        f"- üßë‚Äçüíº Tenant/User: `{user_id}`\n"
        f"- ‚è±Ô∏è Recent window: last {days} days\n"
    )
    sections.append(intro)

    # Overview KPIs
    kpi = (
        "### üìä Cybersecurity Overview\n\n"
        "| Metric | Value |\n"
        "|-------|-------|\n"
        f"| üóÇÔ∏è Total files (all risk levels) | **{total_files}** |\n"
        f"| üö® Total High‚Äërisk files | **{total_high}** |\n"
        f"| ‚è±Ô∏è High‚Äërisk files modified in last {days} days | **{recent_high_count}** |\n"
        "\n> Scope: All breakdowns and lists below are for files with overall_risk_rating == ‚ÄúHigh‚Äù.\n"
    )
    sections.append(kpi)

    # Breakdown sections (High-only)
    sections.append("### üß≠ Breakdown by Data Classification (High‚Äërisk)")
    sections.append(_markdown_table(["Classification", "Count"], cls_rows) if cls_rows else "_No High‚Äërisk classification data._")

    sections.append("\n### üë• Breakdown by Likely Data Subject Area (High‚Äërisk)")
    sections.append(_markdown_table(["Subject Area", "Count"], subj_rows) if subj_rows else "_No High‚Äërisk subject area data._")

    sections.append("\n### üóÑÔ∏è Count by Data Source / Backend (High‚Äërisk)")
    sections.append(_markdown_table(["Data Source", "Count"], src_rows) if src_rows else "_No High‚Äërisk data source info._")

    # Optional: Files in a chosen High-risk source
    if filter_source:
        sections.append(f"\n### üìÅ High‚Äërisk Files in Source: {filter_source}")
        if files_by_source_rows:
            sections.append(_markdown_table(["File Name", "Source", "Classification", "Risk", "Last Modified"], files_by_source_rows))
        else:
            sections.append("_No High‚Äërisk files found for this source._")

    # Unique permission emails (High-only)
    sections.append("\n### üì® Unique Permission Emails (High‚Äërisk)")
    if unique_emails_list:
        sections.append("\n".join(f"- {e}" for e in unique_emails_list))
    else:
        sections.append("_No email-style permissions detected among High‚Äërisk files._")

    # Summary by auditor action (High-only)
    sections.append("\n### üßë‚Äç‚öñÔ∏è Count by Auditor Proposed Action (High‚Äërisk)")
    sections.append(_markdown_table(["Proposed Action", "Count"], action_rows) if action_rows else "_No auditor actions found for High‚Äërisk files._")

    # High-risk by jurisdiction (if present)
    if juris_rows:
        sections.append("\n### üåê High‚Äërisk Files by Jurisdiction")
        sections.append(_markdown_table(["Jurisdiction", "Count"], juris_rows))

    # Unique 50 High-risk file detail list
    sections.append("\n### üßæ High‚Äërisk Files ‚Äî Unique List (By Recency)")
    if high_detail_rows:
        sections.append(_markdown_table(
            ["File Name", "Source", "Classification", "Risk", "Subject Area", "Last Modified"],
            high_detail_rows
        ))
        if total_high > 50:
            sections.append(f"_Showing upto 50 unique High‚Äërisk files out of **{total_high}** total High‚Äërisk files._")
    else:
        sections.append("_No High‚Äërisk file details available to display._")

    # Footer + CSV links
    #hrisk_csv = f"/download_csv?file=HighRisk_{user_id}.csv"
    #allrisk_csv = f"/download_csv?file=AllRisk_{user_id}.csv"
    #sections.append(f"\n---\n[‚¨áÔ∏è Download High Risk CSV]({hrisk_csv})  |  [‚¨áÔ∏è Download All Risk CSV]({allrisk_csv})")

    reply = "\n\n".join(sections)

    followups = [
        {"label": "Show ALL externally shared files", "operation": "cybersec_show_external", "args": {}, "prompt": "List externally shared files."},
        {"label": "Show ALL duplicate files", "operation": "cybersec_find_duplicates", "args": {}, "prompt": "List duplicate files across storages."}
    ]

    return {"reply": reply, "followups": followups}